{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "## Supervised Learning\n",
    "## Project 2: Building a Student Intervention System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the second project of the Machine Learning Engineer Nanodegree! In this notebook, some template code has already been provided for you, and it will be your job to implement the additional functionality necessary to successfully complete this project. Sections that begin with **'Implementation'** in the header indicate that the following block of code will require additional functionality which you must provide. Instructions will be provided for each section and the specifics of the implementation are marked in the code block with a `'TODO'` statement. Please be sure to read the instructions carefully!\n",
    "\n",
    "In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a **'Question X'** header. Carefully read each question and provide thorough answers in the following text boxes that begin with **'Answer:'**. Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide.  \n",
    "\n",
    ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut. In addition, Markdown cells can be edited by typically double-clicking the cell to enter edit mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 - Classification vs. Regression\n",
    "*Your goal for this project is to identify students who might need early intervention before they fail to graduate. Which type of supervised learning problem is this, classification or regression? Why?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This supervised learning problem is one of **classification**, since the desired output is whether (in a binary sense) a student needs early intervention.  \n",
    "This supervised learning problem does not seek to quantify or convey the strength of the need for intervention, which would be a regression, but instead merely flags (or fails to flag) a student as needing early intervention, under the presumption that the school's administration then intervenes commensurate with the student's need.  \n",
    "In machine learning parlance, the desired output here is a **label**, either yes or no, on the **class** of \"Does this student need early intervention\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Data\n",
    "Run the code cell below to load necessary Python libraries and load the student data. Note that the last column from this dataset, `'passed'`, will be our target label (whether the student graduated or didn't graduate). All other columns are features about each student."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student data read successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Read student data\n",
    "student_data = pd.read_csv(\"student-data.csv\")\n",
    "print \"Student data read successfully!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>address</th>\n",
       "      <th>famsize</th>\n",
       "      <th>Pstatus</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>Mjob</th>\n",
       "      <th>Fjob</th>\n",
       "      <th>...</th>\n",
       "      <th>internet</th>\n",
       "      <th>romantic</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>Dalc</th>\n",
       "      <th>Walc</th>\n",
       "      <th>health</th>\n",
       "      <th>absences</th>\n",
       "      <th>passed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>18</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>at_home</td>\n",
       "      <td>teacher</td>\n",
       "      <td>...</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>17</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>at_home</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>15</td>\n",
       "      <td>U</td>\n",
       "      <td>LE3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>at_home</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>15</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>health</td>\n",
       "      <td>services</td>\n",
       "      <td>...</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>16</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  school sex  age address famsize Pstatus  Medu  Fedu     Mjob      Fjob  \\\n",
       "0     GP   F   18       U     GT3       A     4     4  at_home   teacher   \n",
       "1     GP   F   17       U     GT3       T     1     1  at_home     other   \n",
       "2     GP   F   15       U     LE3       T     1     1  at_home     other   \n",
       "3     GP   F   15       U     GT3       T     4     2   health  services   \n",
       "4     GP   F   16       U     GT3       T     3     3    other     other   \n",
       "\n",
       "   ...   internet romantic  famrel  freetime  goout Dalc Walc health absences  \\\n",
       "0  ...         no       no       4         3      4    1    1      3        6   \n",
       "1  ...        yes       no       5         3      3    1    1      3        4   \n",
       "2  ...        yes       no       4         3      2    2    3      3       10   \n",
       "3  ...        yes      yes       3         2      2    1    1      5        2   \n",
       "4  ...         no       no       4         3      2    1    2      5        4   \n",
       "\n",
       "  passed  \n",
       "0     no  \n",
       "1     no  \n",
       "2    yes  \n",
       "3    yes  \n",
       "4    yes  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_data.head()\n",
    "# Get an idea of the shape of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>traveltime</th>\n",
       "      <th>studytime</th>\n",
       "      <th>failures</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>Dalc</th>\n",
       "      <th>Walc</th>\n",
       "      <th>health</th>\n",
       "      <th>absences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>16.696203</td>\n",
       "      <td>2.749367</td>\n",
       "      <td>2.521519</td>\n",
       "      <td>1.448101</td>\n",
       "      <td>2.035443</td>\n",
       "      <td>0.334177</td>\n",
       "      <td>3.944304</td>\n",
       "      <td>3.235443</td>\n",
       "      <td>3.108861</td>\n",
       "      <td>1.481013</td>\n",
       "      <td>2.291139</td>\n",
       "      <td>3.554430</td>\n",
       "      <td>5.708861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.276043</td>\n",
       "      <td>1.094735</td>\n",
       "      <td>1.088201</td>\n",
       "      <td>0.697505</td>\n",
       "      <td>0.839240</td>\n",
       "      <td>0.743651</td>\n",
       "      <td>0.896659</td>\n",
       "      <td>0.998862</td>\n",
       "      <td>1.113278</td>\n",
       "      <td>0.890741</td>\n",
       "      <td>1.287897</td>\n",
       "      <td>1.390303</td>\n",
       "      <td>8.003096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>22.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>75.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              age        Medu        Fedu  traveltime   studytime    failures  \\\n",
       "count  395.000000  395.000000  395.000000  395.000000  395.000000  395.000000   \n",
       "mean    16.696203    2.749367    2.521519    1.448101    2.035443    0.334177   \n",
       "std      1.276043    1.094735    1.088201    0.697505    0.839240    0.743651   \n",
       "min     15.000000    0.000000    0.000000    1.000000    1.000000    0.000000   \n",
       "25%     16.000000    2.000000    2.000000    1.000000    1.000000    0.000000   \n",
       "50%     17.000000    3.000000    2.000000    1.000000    2.000000    0.000000   \n",
       "75%     18.000000    4.000000    3.000000    2.000000    2.000000    0.000000   \n",
       "max     22.000000    4.000000    4.000000    4.000000    4.000000    3.000000   \n",
       "\n",
       "           famrel    freetime       goout        Dalc        Walc      health  \\\n",
       "count  395.000000  395.000000  395.000000  395.000000  395.000000  395.000000   \n",
       "mean     3.944304    3.235443    3.108861    1.481013    2.291139    3.554430   \n",
       "std      0.896659    0.998862    1.113278    0.890741    1.287897    1.390303   \n",
       "min      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "25%      4.000000    3.000000    2.000000    1.000000    1.000000    3.000000   \n",
       "50%      4.000000    3.000000    3.000000    1.000000    2.000000    4.000000   \n",
       "75%      5.000000    4.000000    4.000000    2.000000    3.000000    5.000000   \n",
       "max      5.000000    5.000000    5.000000    5.000000    5.000000    5.000000   \n",
       "\n",
       "         absences  \n",
       "count  395.000000  \n",
       "mean     5.708861  \n",
       "std      8.003096  \n",
       "min      0.000000  \n",
       "25%      0.000000  \n",
       "50%      4.000000  \n",
       "75%      8.000000  \n",
       "max     75.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_data.describe()\n",
    "# Another view of the data, wholly for my own personal benefit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "school        0\n",
       "sex           0\n",
       "age           0\n",
       "address       0\n",
       "famsize       0\n",
       "Pstatus       0\n",
       "Medu          0\n",
       "Fedu          0\n",
       "Mjob          0\n",
       "Fjob          0\n",
       "reason        0\n",
       "guardian      0\n",
       "traveltime    0\n",
       "studytime     0\n",
       "failures      0\n",
       "schoolsup     0\n",
       "famsup        0\n",
       "paid          0\n",
       "activities    0\n",
       "nursery       0\n",
       "higher        0\n",
       "internet      0\n",
       "romantic      0\n",
       "famrel        0\n",
       "freetime      0\n",
       "goout         0\n",
       "Dalc          0\n",
       "Walc          0\n",
       "health        0\n",
       "absences      0\n",
       "passed        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_data.isnull().apply(sum)\n",
    "# Check whether the dataset is missing information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation: Data Exploration\n",
    "Let's begin by investigating the dataset to determine how many students we have information on, and learn about the graduation rate among these students. In the code cell below, you will need to compute the following:\n",
    "- The total number of students, `n_students`.\n",
    "- The total number of features for each student, `n_features`.\n",
    "- The number of those students who passed, `n_passed`.\n",
    "- The number of those students who failed, `n_failed`.\n",
    "- The graduation rate of the class, `grad_rate`, in percent (%).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of students: 395\n",
      "Number of features: 30\n",
      "Number of students who passed: 265\n",
      "Number of students who failed: 130\n",
      "Graduation rate of the class: 67.09%\n"
     ]
    }
   ],
   "source": [
    "# TODO: Calculate number of students\n",
    "n_students = len(student_data)\n",
    "\n",
    "# TODO: Calculate number of features\n",
    "n_features = len(student_data.columns) - 1\n",
    "    # We are told in the prompt that one and only one column is a label,\n",
    "    # hence this must be subtracted from the count.\n",
    "\n",
    "# TODO: Calculate passing students\n",
    "n_passed = sum(student_data[\"passed\"]==\"yes\")\n",
    "\n",
    "# TODO: Calculate failing students\n",
    "n_failed = sum(student_data[\"passed\"]==\"no\")\n",
    "\n",
    "# TODO: Calculate graduation rate\n",
    "grad_rate = float(n_passed) / n_students * 100\n",
    "    # Python 2.7 defaults to integer division.\n",
    "\n",
    "# Print the results\n",
    "print \"Total number of students: {}\".format(n_students)\n",
    "print \"Number of features: {}\".format(n_features)\n",
    "print \"Number of students who passed: {}\".format(n_passed)\n",
    "print \"Number of students who failed: {}\".format(n_failed)\n",
    "print \"Graduation rate of the class: {:.2f}%\".format(grad_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "In this section, we will prepare the data for modeling, training and testing.\n",
    "\n",
    "### Identify feature and target columns\n",
    "It is often the case that the data you obtain contains non-numeric features. This can be a problem, as most machine learning algorithms expect numeric data to perform computations with.\n",
    "\n",
    "Run the code cell below to separate the student data into feature and target columns to see if any features are non-numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature columns:\n",
      "['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu', 'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences']\n",
      "\n",
      "Target column: passed\n",
      "\n",
      "Feature values:\n",
      "  school sex  age address famsize Pstatus  Medu  Fedu     Mjob      Fjob  \\\n",
      "0     GP   F   18       U     GT3       A     4     4  at_home   teacher   \n",
      "1     GP   F   17       U     GT3       T     1     1  at_home     other   \n",
      "2     GP   F   15       U     LE3       T     1     1  at_home     other   \n",
      "3     GP   F   15       U     GT3       T     4     2   health  services   \n",
      "4     GP   F   16       U     GT3       T     3     3    other     other   \n",
      "\n",
      "    ...    higher internet  romantic  famrel  freetime goout Dalc Walc health  \\\n",
      "0   ...       yes       no        no       4         3     4    1    1      3   \n",
      "1   ...       yes      yes        no       5         3     3    1    1      3   \n",
      "2   ...       yes      yes        no       4         3     2    2    3      3   \n",
      "3   ...       yes      yes       yes       3         2     2    1    1      5   \n",
      "4   ...       yes       no        no       4         3     2    1    2      5   \n",
      "\n",
      "  absences  \n",
      "0        6  \n",
      "1        4  \n",
      "2       10  \n",
      "3        2  \n",
      "4        4  \n",
      "\n",
      "[5 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "# Extract feature columns\n",
    "feature_cols = list(student_data.columns[:-1])\n",
    "\n",
    "# Extract target column 'passed'\n",
    "target_col = student_data.columns[-1] \n",
    "\n",
    "# Show the list of columns\n",
    "print \"Feature columns:\\n{}\".format(feature_cols)\n",
    "print \"\\nTarget column: {}\".format(target_col)\n",
    "\n",
    "# Separate the data into feature data and target data (X_all and y_all, respectively)\n",
    "X_all = student_data[feature_cols]\n",
    "y_all = student_data[target_col]\n",
    "\n",
    "# Show the feature information by printing the first five rows\n",
    "print \"\\nFeature values:\"\n",
    "print X_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Feature Columns\n",
    "\n",
    "As you can see, there are several non-numeric columns that need to be converted! Many of them are simply `yes`/`no`, e.g. `internet`. These can be reasonably converted into `1`/`0` (binary) values.\n",
    "\n",
    "Other columns, like `Mjob` and `Fjob`, have more than two values, and are known as _categorical variables_. The recommended way to handle such a column is to create as many columns as possible values (e.g. `Fjob_teacher`, `Fjob_other`, `Fjob_services`, etc.), and assign a `1` to one of them and `0` to all others.\n",
    "\n",
    "These generated columns are sometimes called _dummy variables_, and we will use the [`pandas.get_dummies()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html?highlight=get_dummies#pandas.get_dummies) function to perform this transformation. Run the code cell below to perform the preprocessing routine discussed in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed feature columns (48 total features):\n",
      "['school_GP', 'school_MS', 'sex_F', 'sex_M', 'age', 'address_R', 'address_U', 'famsize_GT3', 'famsize_LE3', 'Pstatus_A', 'Pstatus_T', 'Medu', 'Fedu', 'Mjob_at_home', 'Mjob_health', 'Mjob_other', 'Mjob_services', 'Mjob_teacher', 'Fjob_at_home', 'Fjob_health', 'Fjob_other', 'Fjob_services', 'Fjob_teacher', 'reason_course', 'reason_home', 'reason_other', 'reason_reputation', 'guardian_father', 'guardian_mother', 'guardian_other', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences']\n"
     ]
    }
   ],
   "source": [
    "def preprocess_features(X):\n",
    "    ''' Preprocesses the student data and converts non-numeric binary variables into\n",
    "        binary (0/1) variables. Converts categorical variables into dummy variables. '''\n",
    "    \n",
    "    # Initialize new output DataFrame\n",
    "    output = pd.DataFrame(index = X.index)\n",
    "\n",
    "    # Investigate each feature column for the data\n",
    "    for col, col_data in X.iteritems():\n",
    "        \n",
    "        # If data type is non-numeric, replace all yes/no values with 1/0\n",
    "        if col_data.dtype == object:\n",
    "            col_data = col_data.replace(['yes', 'no'], [1, 0])\n",
    "\n",
    "        # If data type is categorical, convert to dummy variables\n",
    "        if col_data.dtype == object:\n",
    "            # Example: 'school' => 'school_GP' and 'school_MS'\n",
    "            col_data = pd.get_dummies(col_data, prefix = col)  \n",
    "        \n",
    "        # Collect the revised columns\n",
    "        output = output.join(col_data)\n",
    "    \n",
    "    return output\n",
    "\n",
    "X_all = preprocess_features(X_all)\n",
    "print \"Processed feature columns ({} total features):\\n{}\".format(len(X_all.columns), list(X_all.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation: Training and Testing Data Split\n",
    "So far, we have converted all _categorical_ features into numeric values. For the next step, we split the data (both features and corresponding labels) into training and test sets. In the following code cell below, you will need to implement the following:\n",
    "- Randomly shuffle and split the data (`X_all`, `y_all`) into training and testing subsets.\n",
    "  - Use 300 training points (approximately 75%) and 95 testing points (approximately 25%).\n",
    "  - Set a `random_state` for the function(s) you use, if provided.\n",
    "  - Store the results in `X_train`, `X_test`, `y_train`, and `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 300 samples.\n",
      "Testing set has 95 samples.\n",
      "\n",
      "Grad rate of the training set: 67.00%\n",
      "Grad rate of the testing set: 67.37%\n"
     ]
    }
   ],
   "source": [
    "# TODO: Import any additional functionality you may need here\n",
    "\n",
    "# TODO: Set the number of training points\n",
    "num_train = 300\n",
    "\n",
    "# Set the number of testing points\n",
    "num_test = X_all.shape[0] - num_train\n",
    "\n",
    "# TODO: Shuffle and split the dataset into the number of training and testing points above\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_all, y_all, stratify=y_all, train_size = num_train, test_size = num_test, random_state = 42)\n",
    "\n",
    "# Show the results of the split\n",
    "print \"Training set has {} samples.\".format(X_train.shape[0])\n",
    "print \"Testing set has {} samples.\".format(X_test.shape[0])\n",
    "\n",
    "# Verify that the stratification was successful, directly pulled from review\n",
    "print \"\"\n",
    "print \"Grad rate of the training set: {:.2f}%\".format(100 * (y_train == 'yes').mean())\n",
    "print \"Grad rate of the testing set: {:.2f}%\".format(100 * (y_test == 'yes').mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluating Models\n",
    "In this section, you will choose 3 supervised learning models that are appropriate for this problem and available in `scikit-learn`. You will first discuss the reasoning behind choosing these three models by considering what you know about the data and each model's strengths and weaknesses. You will then fit the model to varying sizes of training data (100 data points, 200 data points, and 300 data points) and measure the F<sub>1</sub> score. You will need to produce three tables (one for each model) that shows the training set size, training time, prediction time, F<sub>1</sub> score on the training set, and F<sub>1</sub> score on the testing set.\n",
    "\n",
    "**The following supervised learning models are currently available in** [`scikit-learn`](http://scikit-learn.org/stable/supervised_learning.html) **that you may choose from:**\n",
    "- Gaussian Naive Bayes (GaussianNB)\n",
    "- Decision Trees\n",
    "- Ensemble Methods (Bagging, AdaBoost, Random Forest, Gradient Boosting)\n",
    "- K-Nearest Neighbors (KNeighbors)\n",
    "- Stochastic Gradient Descent (SGDC)\n",
    "- Support Vector Machines (SVM)\n",
    "- Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 - Model Application\n",
    "*List three supervised learning models that are appropriate for this problem. For each model chosen*\n",
    "- Describe one real-world application in industry where the model can be applied. *(You may need to do a small bit of research for this â€” give references!)* \n",
    "- What are the strengths of the model; when does it perform well? \n",
    "- What are the weaknesses of the model; when does it perform poorly?\n",
    "- What makes this model a good candidate for the problem, given what you know about the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three supervised learning models that are appropriate for this problem are **Decision Trees**, **Gradient Boosting**, and **K-Nearest Neighbors**, discussed below.\n",
    "<h3><font color=\"darkgreen\">Decision Trees</font></h3>\n",
    "**Decision Trees** attempt to classify the data by playing 20 questions.  This algorithm works best when there are easily-identified clefts along a feature, and the features are fairly independent of each other.  This allows each question to generate maximal information gain, and reduces having to ask the same or a similar question again later.\n",
    "<h4>Uses</h4>\n",
    "Probably the most impressive collection of scientific and industrial applications of Decision Trees is found in [this 1995 write-up](http://www.cbcb.umd.edu/~salzberg/docs/murthy_thesis/survey/node32.html) by big-data rockstar Sreerama K. Murthy, which includes applications in the fields of astronomy, manufacturing, pharmacology, and medicine.  One particular such application was to [classify objects](http://iopscience.iop.org/article/10.1086/133551/meta) in Hubble Space Telescope images as \"stars, galaxies, cosmic rays, plate defects, and other types of objects ... with over 95% accuracy using data from a single, upaired image\".\n",
    "<h4>Strengths</h4>\n",
    "Decision Trees are conceptually appealing; if we have a binary classification for which there are a host of necessary and sufficient conditions, we can concisely represent this classification exercise as a decision tree.  \n",
    "Similarly, we can represent arbitrary complexity with decision trees (although we may not always want to do so).  Finally, it's possible to tune a Decision Tree to the \"right\" degree of complexity for the dataset.\n",
    "<h4>Weaknesses</h4>\n",
    "Fundamentally, a decision tree is not a good solution when the label boundaries are not feature-axis-aligned.  That is, if y = x is the best classification boundary, a decision tree would need many tiny \"stairs\" to capture that fact.  This would result in a perfectly-functional decision tree, but which requires traversing a computationally large number of nodes to arrive at the correct classification.  \n",
    "Further, the strength of being able to tune a Decision Tree to the \"right\" degree of complexity is, in the same breath, a weakness that allows for a Decision Tree to be improperly tuned, either leading to overgeneralizations that miss most of the training data or to hyper-specializations that overfit at the expense of predictive power.  \n",
    "<h4>Application to Student Intervention</h4>\n",
    "Here, I would presume that certain features included with this dataset have meaningful classification clefts.  For example, if you've missed every day of school, I feel comfortable predicting that you need intervention to pass.  Similarly, if your weekday alcohol consumption is high, that probably means that intervention is needed to pass.  Other intuitive-feeling clefts would be low weekly study time without being a genious, very bad family relationships, and not wanting to take higher education.  \n",
    "Now, the decision tree algorithm doesn't take _my_ input for what are useful features along which to cleave, but I do think those axis-aligned split points exist, and so I would guess that the decision tree algorithm is a good classifier to train on this dataset.\n",
    "\n",
    "<h3><font color=\"darkgreen\">Gradient Boosting</font></h3>\n",
    "The **Gradient Boosting** algorithm tries to learn from what it gets wrong, by iteratively doing slightly better than random chance at guessing on the leftover error.  That doesn't sound terribly impressive, but in practice it has been quite useful.  \n",
    "This algorithm is very similar to the AdaBoost algorithm discussed in lecture, with a technical difference in _how_ it focuses on leftover error (i.e. gradient boost employs gradient descent on the sum of squared residual errors, while AdaBoost tweaks the \"importance\" of datapoints based on the correctness of the previous classification). \n",
    "<h4>Uses</h4>\n",
    "Gradient Boosting has been used successfully for neurorobotics applications, specifically in mapping [EMG](https://en.wikipedia.org/wiki/Electromyography) signals to [a robotic arm](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3885826/) by watching the human participant and measuring brain activity.  Gradient Boosting is also used extensively in facial recognition, whether in [realtime](http://accord-framework.net/docs/html/T_Accord_Vision_Detection_HaarObjectDetector.htm), or to catalog who was present in a [video-steam](http://link.springer.com/chapter/10.1007%2F978-3-642-23123-0_14).  Another visual application of the Gradient Boosting approach is to [detect crop blight](http://imperialjournals.com/index.php/IJIR/article/view/465/449).\n",
    "<h4>Strengths</h4>\n",
    "Being an ensemble method, Gradient Boosting is versatile in that it can use any of a wide variety of base classifiers, from the common decision tree to the inscrutible neural network.  \n",
    "As suggested by the above applications, Gradient Boosting with decision trees has the advantage of being able to work well without the manual injection of domain knowledge.  The base classifier can of course still be poorly-suited to the classification task at hand, but gradient boosting typically makes the best of whatever classifier it's given.  \n",
    "Gradient Boosting is also the algorithm [_du jour_](https://github.com/dmlc/xgboost/tree/master/demo#machine-learning-challenge-winning-solutions) for Kaggle.  That's not a rigorous examination of the merits of the algorithm, but it does speak to the machine learning community's adoption of it, and thus to the likelihood of (1) finding a colleague who understands the algorithm, of (2) finding a robust and well-tested software implementation of the algorithm, and of (3) future practical and theoretical work being done on this algorithm.\n",
    "<h4>Weaknesses</h4>\n",
    "Gradient Boosting is not magic; it is still subject to the constraints of garbage-in:garbage-out.  That is, it requires a weak classifier even to work, and this is not an easy requirement to meet.\n",
    "The algorithm also [doesn't](http://www.cs.columbia.edu/~rocco/Public/mlj9.pdf) handle random noise well.  That is, boosting algorithms tend to kind of obsess over data that it's not correctly classifying, without any mechanism to ignore any one label as erroneous.  \n",
    "This algorithm is also memory-intensive.  This is not surprising, since it has to remember at least some of perhaps thousands of individual weak classifiers to comprise the composite classifier that is the goal of boosting.  While it's a truism that memory is cheap, it's also the case that we will find increasingly larger problems to keep pace with the advances in memory capacity and access speed.\n",
    "<h4>Application to Student Intervention</h4>\n",
    "Here, the idea is that individual, say, Decision Tree classifiers, will be sufficiently good to be classified as \"weak\" on the student intervention dataset.  According to the algorithmic requirements of gradient boosting, that's all that's needed.  The assumption is of course that the Gradient Boosting algorithm will appropriately stitch together individual Decision Tree classifiers to minimize the sum of squared prediction errors, and in so doing, produce a composite classifier with a high degree of accuracy on the test set.\n",
    "\n",
    "<h3><font color=\"darkgreen\">K-Nearest Neighbors</font></h3>\n",
    "**K-Nearest Neighbors** attempts to provide a classification label based off of some measure of **similarity**.  \n",
    "That is, if something has feathers, quacks, and really loves bread, it's probably a duck, even if that's a bit of an oversimplification.\n",
    "<h4>Uses</h4>\n",
    "KNeighbors has been used successfully in [medical research](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2243774/) and [credit card fraud detection](https://www.researchgate.net/publication/236962626_credit_card_fraud_detection_using_anti_k-nearest_algorithm), but could reasonably be expected to produce good results whenever there is a diffusion of datapoints in a particular feature space (read: enough data), within some contiguous space-saving boundary (that is, not highly intermixed).\n",
    "<h4>Strengths</h4>\n",
    "Because K-Nearest Neighbors depends merely on some measure of similarity, it is possible for a KNeighbors classifier to accurately capture arbitrary structural complexity within the data, without the need to inject that domain knowledge in the form of a kernel trick, polynomial degree, or some other function.  \n",
    "This allows a KNeighbors algorithm to merely \"discover\" structure in the data, once a measure of similarity is supplied.  \n",
    "When datapoints which are similar in the featurespace are also similar in the labelspace, KNeighbors is an excellent algorithm.\n",
    "<h4>Weaknesses</h4>\n",
    "KNeighbors has difficulty with edge cases, such as whether someone with really high cholesterol who exercises regularly and eats well would be at risk for heart disease, or whether a particular entry in the [iris dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set#/media/File:Iris_dataset_scatterplot.svg) with median sepal length and sepal width is Iris virginica or Iris versicolor, or whether someone who is socially conservative but supports broad spending on social programs is a \"Republican\" or \"Democrat\".  \n",
    "Further, KNeighbors suffers from the **curse of dimensionality**, where the number of datapoints needed to properly train a KNeighbors algorithm grows exponentially with the number of features (dimensions) given to the classifier.  That is, KNeighbors is really only feasible with less than 10 input features, unless the number of input data is billions upon billions.  \n",
    "Here, we have 300 training points.  That suggests we can probably only hope to use 5 features max, and those may need to be combined / projected featurespaces.\n",
    "<h4>Application to Student Intervention</h4>\n",
    "Here, the assumption is that there is some similarity measure between students who are numerically close to each other in some representation of a feature space.  \n",
    "That's probably not as weird as it sounds for the clear-cut cases, but the concern of course is whether there is sufficient \"distance\" between passing and non-passing students to allow for proper classification.  If there's significant spatial overlap, it may be difficult for KNeighbors to correctly classify border cases.  This increases the percentage of students who would be susceptible to Type 1 neglect or Type 2 over-worry.  \n",
    "In the alternative, it could be that there is a significant spatial boundary between students who require and students who do not require early intervention, in which case the KNeighbors algorithm should function exceptionally well on this dataset.\n",
    "<br><br>\n",
    "<font color=\"darkred\">_Discarded candidates_</font>:\n",
    "<font color=\"darkred\">**Gaussian Naive Bayes**</font> is an attempt to provide classifications based on the \"naive\" assumption of conditional independence between the input features.  Here, that seems inappropriate, considering that many of the features are likely outgrowths of other, simpler features (e.g. famsize, address, Medu, Fedu, Mjob, Fjob, activities, nursery, internet, goout, health, etc. likely co-vary with parental income).\n",
    "\n",
    "<font color=\"darkred\">**Stochastic Gradient Descent**</font> models attempt to discover the set of parameters which minimize the error of the prediction.  However, they do not provide information on what the error \"surface\" looks like.  That is, (1) there is no guarantee that the minimum which is found is the global, rather than merely a local, minimum, (2) there is no guarantee that the minimum which is found is even \"good\" relative to other regional minimums, and (3) it could be that the error surface is like a rolling sand dune, where no particular minimum is good, or even much better than other possible minimums.  It is possible (and quite effective), through multiple randomized initiations, to merely take thousands of samples of the existing local minimums, and to compare them to each other, in an attempt to locate the global minimum, but this may be computationally very expensive.\n",
    "\n",
    "<font color=\"darkred\">**Support Vector Machines**</font> rely on linear or near-linear separability of the data (as represented in some-dimensional space).  If the data is not linearly separable in the default space, it requires a transformation via a kernel method into a space in which it is linearly separable.  The main problem with that fact is that here I don't know, _a priori_, how the features affect whether a student passes or doesn't.  So, if the default spatial representation of the data results in a poorly-performing SVM, I wouldn't actually know which kernel method to apply to increase performance, even acknowledging that there probably is one which would allow for excellent SVM performance.  Visualizing data in a three-dimensional feature space is hard enough, but here we start with a 30-dimensional feature space, which my tiny human brain struggles to comprehend visually.\n",
    "\n",
    "<font color=\"darkred\">**Logistic Regression**</font> (actually a classification) is a lot like linear regression, except that the continuous variable it outputs is the _likelihood_ of whether a datapoint does or doesn't (in a binary sense) have a label.  Here, that would mean that a properly-implemented Logistic Regression would output the likelihood that a student does or doesn't need early intervention.  The output would be useful, but the underlying assumption is that the variables in the feature space have a consistent linear relation to our output label, which for many of the labels seems like a ludicrous assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "Run the code cell below to initialize three helper functions which you can use for training and testing the three supervised learning models you've chosen above. The functions are as follows:\n",
    "- `train_classifier` - takes as input a classifier and training data and fits the classifier to the data.\n",
    "- `predict_labels` - takes as input a fit classifier, features, and a target labeling and makes predictions using the F<sub>1</sub> score.\n",
    "- `train_predict` - takes as input a classifier, and the training and testing data, and performs `train_clasifier` and `predict_labels`.\n",
    " - This function will report the F<sub>1</sub> score for both the training and testing data separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_classifier(clf, X_train, y_train):\n",
    "    ''' Fits a classifier to the training data. '''\n",
    "    \n",
    "    # Start the clock, train the classifier, then stop the clock\n",
    "    start = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end = time()\n",
    "    \n",
    "    # Print the results\n",
    "    print \"Trained model in {:.4f} seconds\".format(end - start)\n",
    "\n",
    "    \n",
    "def predict_labels(clf, features, target):\n",
    "    ''' Makes predictions using a fit classifier based on F1 score. '''\n",
    "    \n",
    "    # Start the clock, make predictions, then stop the clock\n",
    "    start = time()\n",
    "    y_pred = clf.predict(features)\n",
    "    end = time()\n",
    "    \n",
    "    # Print and return results\n",
    "    print \"Made predictions in {:.4f} seconds.\".format(end - start)\n",
    "    return f1_score(target.values, y_pred, pos_label='yes')\n",
    "\n",
    "\n",
    "def train_predict(clf, X_train, y_train, X_test, y_test):\n",
    "    ''' Train and predict using a classifer based on F1 score. '''\n",
    "    \n",
    "    # Indicate the classifier and the training set size\n",
    "    print \"Training a {} using a training set size of {}. . .\".format(clf.__class__.__name__, len(X_train))\n",
    "    \n",
    "    # Train the classifier\n",
    "    train_classifier(clf, X_train, y_train)\n",
    "    \n",
    "    # Print the results of prediction for both training and testing\n",
    "    print \"F1 score for training set: {:.4f}.\".format(predict_labels(clf, X_train, y_train))\n",
    "    print \"F1 score for test set: {:.4f}.\".format(predict_labels(clf, X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation: Model Performance Metrics\n",
    "With the predefined functions above, you will now import the three supervised learning models of your choice and run the `train_predict` function for each one. Remember that you will need to train and predict on each classifier for three different training set sizes: 100, 200, and 300. Hence, you should expect to have 9 different outputs below â€” 3 for each model using the varying training set sizes. In the following code cell, you will need to implement the following:\n",
    "- Import the three supervised learning models you've discussed in the previous section.\n",
    "- Initialize the three models and store them in `clf_A`, `clf_B`, and `clf_C`.\n",
    " - Use a `random_state` for each model you use, if provided.\n",
    " - **Note:** Use the default settings for each model â€” you will tune one specific model in a later section.\n",
    "- Create the different training set sizes to be used to train each model.\n",
    " - *Do not reshuffle and resplit the data! The new training points should be drawn from `X_train` and `y_train`.*\n",
    "- Fit each model with each training set size and make predictions on the test set (9 in total).  \n",
    "**Note:** Three tables are provided after the following code cell which can be used to store your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a DecisionTreeClassifier using a training set size of 100. . .\n",
      "Trained model in 0.0056 seconds\n",
      "Made predictions in 0.0005 seconds.\n",
      "F1 score for training set: 1.0000.\n",
      "Made predictions in 0.0003 seconds.\n",
      "F1 score for test set: 0.6452.\n",
      "\n",
      "Training a DecisionTreeClassifier using a training set size of 200. . .\n",
      "Trained model in 0.0024 seconds\n",
      "Made predictions in 0.0003 seconds.\n",
      "F1 score for training set: 1.0000.\n",
      "Made predictions in 0.0003 seconds.\n",
      "F1 score for test set: 0.7258.\n",
      "\n",
      "Training a DecisionTreeClassifier using a training set size of 300. . .\n",
      "Trained model in 0.0034 seconds\n",
      "Made predictions in 0.0004 seconds.\n",
      "F1 score for training set: 1.0000.\n",
      "Made predictions in 0.0003 seconds.\n",
      "F1 score for test set: 0.6838.\n",
      "\n",
      "Training a GradientBoostingClassifier using a training set size of 100. . .\n",
      "Trained model in 0.0680 seconds\n",
      "Made predictions in 0.0007 seconds.\n",
      "F1 score for training set: 1.0000.\n",
      "Made predictions in 0.0006 seconds.\n",
      "F1 score for test set: 0.7500.\n",
      "\n",
      "Training a GradientBoostingClassifier using a training set size of 200. . .\n",
      "Trained model in 0.0891 seconds\n",
      "Made predictions in 0.0010 seconds.\n",
      "F1 score for training set: 0.9924.\n",
      "Made predictions in 0.0006 seconds.\n",
      "F1 score for test set: 0.7313.\n",
      "\n",
      "Training a GradientBoostingClassifier using a training set size of 300. . .\n",
      "Trained model in 0.1198 seconds\n",
      "Made predictions in 0.0019 seconds.\n",
      "F1 score for training set: 0.9781.\n",
      "Made predictions in 0.0006 seconds.\n",
      "F1 score for test set: 0.7463.\n",
      "\n",
      "Training a KNeighborsClassifier using a training set size of 100. . .\n",
      "Trained model in 0.0009 seconds\n",
      "Made predictions in 0.0021 seconds.\n",
      "F1 score for training set: 0.8252.\n",
      "Made predictions in 0.0018 seconds.\n",
      "F1 score for test set: 0.7586.\n",
      "\n",
      "Training a KNeighborsClassifier using a training set size of 200. . .\n",
      "Trained model in 0.0010 seconds\n",
      "Made predictions in 0.0049 seconds.\n",
      "F1 score for training set: 0.8097.\n",
      "Made predictions in 0.0027 seconds.\n",
      "F1 score for test set: 0.7857.\n",
      "\n",
      "Training a KNeighborsClassifier using a training set size of 300. . .\n",
      "Trained model in 0.0013 seconds\n",
      "Made predictions in 0.0106 seconds.\n",
      "F1 score for training set: 0.8539.\n",
      "Made predictions in 0.0039 seconds.\n",
      "F1 score for test set: 0.8138.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Import the three supervised learning models from sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# TODO: Initialize the three models\n",
    "clf_A = DecisionTreeClassifier(random_state=42)\n",
    "clf_B = GradientBoostingClassifier(random_state=42)\n",
    "clf_C = KNeighborsClassifier()\n",
    "    # Presumes, without justification, NNeighbors = 5\n",
    "\n",
    "# TODO: Set up the training set sizes\n",
    "X_train_100 = X_train[:100]\n",
    "y_train_100 = y_train[:100]\n",
    "\n",
    "X_train_200 = X_train[:200]\n",
    "y_train_200 = y_train[:200]\n",
    "\n",
    "X_train_300 = X_train\n",
    "y_train_300 = y_train\n",
    "\n",
    "# TODO: Execute the 'train_predict' function for each classifier and each training set size\n",
    "# train_predict(clf, X_train, y_train, X_test, y_test)\n",
    "for clf in [clf_A, clf_B, clf_C]:\n",
    "    for xtrain, ytrain in [(X_train_100,y_train_100),\n",
    "                           (X_train_200,y_train_200),\n",
    "                           (X_train_300,y_train_300)]:\n",
    "        train_predict(clf, xtrain, ytrain, X_test, y_test)\n",
    "        print\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabular Results\n",
    "Edit the cell below to see how a table can be designed in [Markdown](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet#tables). You can record your results from above in the tables provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Classifer 1 - DecisionTreeClassifier**  \n",
    "\n",
    "| Training Set Size | Training Time | Prediction Time (test) | F1 Score (train) | F1 Score (test) |\n",
    "| :---------------: | :---------------------: | :--------------------: | :--------------: | :-------------: |\n",
    "| 100               |        0.0022           |     0.0006             |   1.0000         |    0.6452       |\n",
    "| 200               |        0.0025           |     0.0006             |   1.0000         |    0.7258       |\n",
    "| 300               |        0.0060           |     0.0005             |   1.0000         |    0.6838       |\n",
    "\n",
    "** Classifer 2 - GradientBoostClassifier**  \n",
    "\n",
    "| Training Set Size | Training Time | Prediction Time (test) | F1 Score (train) | F1 Score (test) |\n",
    "| :---------------: | :---------------------: | :--------------------: | :--------------: | :-------------: |\n",
    "| 100               |     0.0785              |    0.0007              |    1.0000        |   0.7500        |\n",
    "| 200               |     0.0893              |    0.0010              |    0.9924        |   0.7313        |\n",
    "| 300               |     0.1195              |    0.0020              |    0.9781        |   0.7463        |\n",
    "\n",
    "** Classifer 3 - KNeighborsClassifier**  \n",
    "\n",
    "| Training Set Size | Training Time | Prediction Time (test) | F1 Score (train) | F1 Score (test) |\n",
    "| :---------------: | :---------------------: | :--------------------: | :--------------: | :-------------: |\n",
    "| 100               |     0.0007              |    0.0020              |   0.8252         |  0.7586         |\n",
    "| 200               |     0.0009              |    0.0051              |   0.8097         |  0.7857         |\n",
    "| 300               |     0.0012              |    0.0106              |   0.8539         |  0.8138         |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the Best Model\n",
    "In this final section, you will choose from the three supervised learning models the *best* model to use on the student data. You will then perform a grid search optimization for the model over the entire training set (`X_train` and `y_train`) by tuning at least one parameter to improve upon the untuned model's F<sub>1</sub> score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 - Choosing the Best Model\n",
    "*Based on the experiments you performed earlier, in one to two paragraphs, explain to the board of supervisors what single model you chose as the best model. Which model is generally the most appropriate based on the available data, limited resources, cost, and performance?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal here is to find the best-performing model.\n",
    "\n",
    "That goal is similar to finding the best-performing student:  We ask the model to complete a test, then we score it on how well it did.  The test is similar to, but not the same as, the study materials we gave to the model ahead of time, because we want the model to be able to generalize its lessons outside of specific problems we assigned it.\n",
    "\n",
    "Score-wise, the \"best\" model in the preliminary assessment is the **KNeighborsClassifier**, which scores around 81% (way better than random guessing!).  However, **GradientBoostingClassifier** is close behind at ~75% on a preliminary assessment.  Given the number of hyperparameters left to tune away from the default settings, scores this close should be interpreted to signal a rough equivalency in performance capability, and likely **not** as a definitive answer as which model will perform best after tuning.\n",
    "\n",
    "Other valid considerations are how expensive it is to \"teach\" the model, and how expensive it is for the model to provide us with a meaningful answer.  In this application, this model will likely be \"taught\" once, then asked for meaningful answers numerous times.  So here, we care way more about the expense of the answer than we do about the expense of the learning.\n",
    "\n",
    "In this respect, the **GradientBoostingClassifier** is the clear winner: its prediction time (the expense of the answer) is ~1/5 the prediction time of the KNeighborsClassifier.  The training time (the \"learning\" expense) of the GradientBoostingClassifier is ~100x that of the KNeighborsClassifier, but that's a one-time occurrence and thus is far less of a concern than the expense of an answer.\n",
    "\n",
    "Thus, **GradientBoostingClassifier** is presumed to be the best model, as its accuracy is roughly equivalent to the KNeighborsClassifier, and as its prediction time is a fraction of that algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 - Model in Layman's Terms\n",
    "*In one to two paragraphs, explain to the board of directors in layman's terms how the final model chosen is supposed to work. Be sure that you are describing the major qualities of the model, such as how the model is trained and how the model makes a prediction. Avoid using advanced mathematical or technical jargon, such as describing equations or discussing the algorithm implementation.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Motivation and Assumptions</h4>\n",
    "We want all of the students to graduate.  A major obstacle to graduation is not receiving timely academic intervention.  Knowing earlier which students need intervention to pass is thus a top priority.\n",
    "\n",
    "By applying machine learning techniques, we are able to accurately predict which students require academic intervention in order to pass, solely by examining information from the student's academic record supplemented with information provided by a student survey.\n",
    "\n",
    "This prediction is possible because whether or not a student needs intervention is not random, but rather depends quantifiably on a variety of other factors such as parental involvement and education, extracurricular activities, school absences, etc.  By examining which students have failed in the past, this model is able to understand which students are currently at risk of failing.\n",
    "\n",
    "At regular intervals, this model will examine each enrolled student for signs that the student may need intervention in order to pass.  The model will flag students that are at risk of failing without intervention, so that the administration can intervene before it is too late to help the student.\n",
    "\n",
    "Exactly how this occurs is complicated, but clever.\n",
    "\n",
    "<h4>A Guessing Game</h4>\n",
    "We can make guesses about which factors determine whether a student needs intervention, and exactly how they combine to do so.\n",
    "\n",
    "In fact, we can make our first guess _right now_ about how the different student features generally map to needing or not needing intervention: \"Everyone who has failed at least one class before needs an intervention in order to pass, and everyone who hasn't failed a class before does not need an intervention in order to pass.\"  \n",
    "\n",
    "Or, maybe it's that \"Everyone who has 10 or more absences needs intervention in order to pass, and everyone with less than 10 absences doesn't need intervention in order to pass.\"\n",
    "\n",
    "Or, maybe it's a combination of the above two, or more than that.\n",
    "\n",
    "It's not clear at present, so let's talk about how to tell how well we're guessing.\n",
    "\n",
    "<h4>Getting Good at the Guessing Game</h4>\n",
    "We evaluate our guess by looking at how our predictions for who needed intervention compares to who actually needed intervention.  Specifically, for each student, we subtract what we should have predicted from what we did predict, then we square the difference.\n",
    "\n",
    "Squaring the difference is a bit of cleverness, which allows us to know along which \"[direction](https://www.youtube.com/watch?v=eikJboPQDT0)\" to change our guess along in order to make less-bad predictions.  Specifically, this is accomplished through the computation of the gradient at that guess-point in the guess-space.  While the gradient points up-hill, here we're looking to minimize our squared error, so we just go the exact opposite direction of the gradient. \n",
    "\n",
    "Since we've figured out the direction to tweak our guessing so that we make better guesses, we then take what we hope is an appropriately-sized step in that direction.  It's possible we're stepping too large (in which case we might overshoot the best-guess-point we're aiming at), and it's also possible that we're stepping too small (in which case it's going to take us a loooong time to get where we're going, but we're definitely not going to miss it).\n",
    "\n",
    "Eventually, we'll arrive at the best guess in that area.  \n",
    "\n",
    "Perfect, right?  Well, not exactly; we don't know whether it's the best guess we could possibly make, and there's still some predictions that we're getting way wrong.  Is there anything we can do to \"boost\" our performance on the \"residual\" errors?\n",
    "\n",
    "<h4>Boosting our Performance</h4>\n",
    "Weirdly, yes there is, and it's called \"boosting.\"  What we've done above is identified a guess that provides us with some accurate predictions about whether a student needs intervention, but there are some predictions it's still wrong about.  Let's call this leftover inaccuracy \"remaining\" error, and in fact, let's play our guessing game again, but this time just playing with the \"residual\" error.\n",
    "\n",
    "Since we were able to generate accuracy that was better than totally random guessing with the student data in the first game, it must also be the case that we'll be able to generate accuracy with the residual error from the first game.  \n",
    "Powerfully, the best guess we find from our second game will help us improve our answers to the first game, because the second game is basically [subtracting out](https://en.wikipedia.org/wiki/Gradient_boosting#Informal_introduction) the errors from the first game, and so can be added back in to increase predictive accuracy.  \n",
    "And we can continue to do this for a third game, and a fourth game, and a fifth game, and so on until we've gotten every last bit of predictive accuracy out of our guesses.  \n",
    "This doesn't mean we'll necessarily make perfect predictions on new students in the future, but it does mean that we won't leave any accuracy behind.\n",
    "\n",
    "<h4>A Step-by-Step Recap of the Guessing Game</h4>\n",
    "<ul>\n",
    "<li>Define a Loss Function (this is the squared difference between prediction and reality, summed together for each student).\n",
    "<li>Our starting game is the entire student data set.\n",
    "</ul>\n",
    "<ol>\n",
    "<li>For each game (via GradientBoostingClassifier):\n",
    "<ol>\n",
    "<li>Define a hypothesis (this is our \"guess\").\n",
    "<li>Update the hypothesis until it's as good as possible (via Gradient Descent).\n",
    "<ol>\n",
    "<li>Calculate the negative gradient of the loss function defined above (this indicates the \"direction\" we should change our guess towards).\n",
    "<li>If the negative gradient isn't small enough, make a small change to the parameters of the hypothesis in the direction calculated above.\n",
    "<li>If the negative gradient is small enough, we've reached the best guess for this game, and we're done with Gradient Descent.\n",
    "</ol>\n",
    "<li>Save this hypothesis, it's a good one, and we may \"boost\" it through subsequent iterations of games.\n",
    "</ol>\n",
    "<li>Subtract reality from our predictions for each student; this is our residual, and it will form the start of the next game.\n",
    "<li>If the previous best-hypothesis helped our predictions, \"boost\" it.\n",
    "<ol><li>Using the \"residual\" calculated above as the data, return to 1. above, and play another game.</ol>\n",
    "<li>If the previous best-hypothesis did not help our predictions, we're done boosting, and done with the GradientBoostingClassifier algorithm.\n",
    "</ol>\n",
    "<ul><li>Return the best hypothesis, along with the \"boosted\" hypotheses we calculated from its subsequent residuals.  This is our finished GradientBoostingClassifier, and will be used to predict whether or not a student needs intervention in order to pass.</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation: Model Tuning\n",
    "Fine tune the chosen model. Use grid search (`GridSearchCV`) with at least one important parameter tuned with at least 3 different values. You will need to use the entire training set for this. In the code cell below, you will need to implement the following:\n",
    "- Import [`sklearn.grid_search.gridSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html) and [`sklearn.metrics.make_scorer`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html).\n",
    "- Create a dictionary of parameters you wish to tune for the chosen model.\n",
    " - Example: `parameters = {'parameter' : [list of values]}`.\n",
    "- Initialize the classifier you've chosen and store it in `clf`.\n",
    "- Create the F<sub>1</sub> scoring function using `make_scorer` and store it in `f1_scorer`.\n",
    " - Set the `pos_label` parameter to the correct value!\n",
    "- Perform grid search on the classifier `clf` using `f1_scorer` as the scoring method, and store it in `grid_obj`.\n",
    "- Fit the grid search object to the training data (`X_train`, `y_train`), and store it in `grid_obj`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done 104 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=2)]: Done 704 tasks      | elapsed:   24.3s\n",
      "[Parallel(n_jobs=2)]: Done 1000 out of 1000 | elapsed:   33.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made predictions in 0.0018 seconds.\n",
      "Tuned model has a training F1 score of 0.8621.\n",
      "Made predictions in 0.0007 seconds.\n",
      "Tuned model has a testing F1 score of 0.7945.\n",
      "Tuned model has the following parameters: {'n_estimators': 69, 'loss': 'exponential', 'learning_rate': 0.016999999999999994, 'max_depth': 44, 'min_samples_leaf': 24}\n",
      "Tuned model has the following cross-validated F1 score: 0.834167559335\n"
     ]
    }
   ],
   "source": [
    "# TODO: Import 'GridSearchCV' and 'make_scorer'\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# TODO: Create the parameters list you wish to tune\n",
    "parameters = {\n",
    "    \"learning_rate\": np.arange(0.01, 0.02, 0.001)\n",
    "    ,\"n_estimators\": range(60, 100, 1)\n",
    "    ,\"max_depth\": range(30, 60, 1)\n",
    "    ,\"min_samples_leaf\": range(20, 100, 1)\n",
    "    #,\"max_features\": np.arange(0.6, 1.0, 0.01)\n",
    "    #,\"min_samples_split\": range(1, 250, 1)\n",
    "    ,\"loss\":[\"deviance\",\"exponential\"]\n",
    "    #,\"min_weight_fraction_leaf\": np.arange(0.0, 0.2, 0.01)\n",
    "    #,\"subsample\": np.arange(0.97, 0.85, -0.0033)\n",
    "    #,\"max_leaf_nodes\": range(2,1000,10) \n",
    "}\n",
    "\n",
    "# TODO: Initialize the classifier\n",
    "clf = GradientBoostingClassifier(random_state = 42)\n",
    "\n",
    "# TODO: Make an f1 scoring function using 'make_scorer' \n",
    "f1_scorer = make_scorer(f1_score, pos_label=\"yes\")\n",
    "\n",
    "# TODO: Perform grid search on the classifier using the f1_scorer as the scoring method\n",
    "grid_obj = RandomizedSearchCV(clf, parameters, n_iter=100, scoring=f1_scorer, verbose=1, n_jobs=1, cv=10)\n",
    "\n",
    "# TODO: Fit the grid search object to the training data and find the optimal parameters\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Get the estimator\n",
    "clf = grid_obj.best_estimator_\n",
    "\n",
    "# Report the final F1 score for training and testing after parameter tuning\n",
    "print \"Tuned model has a training F1 score of {:.4f}.\".format(predict_labels(clf, X_train, y_train))\n",
    "print \"Tuned model has a testing F1 score of {:.4f}.\".format(predict_labels(clf, X_test, y_test))\n",
    "\n",
    "# Report the parameters of the best estimator\n",
    "print \"Tuned model has the following parameters: \" + str(grid_obj.best_params_)\n",
    "print \"Tuned model has the following cross-validated F1 score: \" + str(grid_obj.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5 - Final F<sub>1</sub> Score\n",
    "*What is the final model's F<sub>1</sub> score for training and testing? How does that score compare to the untuned model?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final model's F<sub>1</sub> score for training was 0.8621.  \n",
    "The final model's F<sub>1</sub> score for testing was 0.7945.\n",
    "\n",
    "(see the below cell for the output of a variety of randomized trials with different parameters, each of which had approximately the same performance)\n",
    "\n",
    "Actually, this is <font color=\"darkred\">slightly worse</font> than the testing score for the best untuned model (KNeighbors, whose F<sub>1,testing</sub> score was 0.8138).  \n",
    "The performance on the training set was worse, but that's not really the point of the exercise, although it does set a \"ceiling\" of sorts on the possible additional performance that can be gained.\n",
    "\n",
    "It appears, then, that for this classification exercise, **KNeighbors** may be better suited than is the GradientBoostingClassifier (see subsequent cells for further proof of this assertion).\n",
    "\n",
    "In a pure grid search scenario, it's unclear whether the final model is stuck in a local minimum of performance, in which case there are other pockets of the parameter-space that would lead to better performance on the testing set.  \n",
    "However, by employing a RandomizedSearchCV, and repeating the trial numerous times, the search has a much better chance of locating high-performing local (or even global) minimums of performance.  \n",
    "Hence, here I'm fairly confident that there is no set of parameters for the GradientBoostingClassifier which would yield better performance.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>GradientBoostingClassifier Trial Results</h3>\n",
    "Made predictions in 0.0015 seconds.  \n",
    "Tuned model has a training F1 score of 0.8494.  \n",
    "Made predictions in 0.0011 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.7770.  \n",
    "Tuned model has the following parameters: {'loss': 'exponential', 'max_features': 0.78000000000000003, 'learning_rate': 0.017000000000000001, 'min_samples_leaf': 22, 'n_estimators': 100, 'subsample': 0.79999999999999982, 'min_samples_split': 64, 'max_depth': 19}  \n",
    "Tuned model has the following cross-validated F1 score: 0.828528408451  \n",
    "  \n",
    "Made predictions in 0.0057 seconds.  \n",
    "Tuned model has a training F1 score of 0.8419.  \n",
    "Made predictions in 0.0029 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.8028.  \n",
    "Tuned model has the following parameters: {'loss': 'deviance', 'max_features': 0.5, 'learning_rate': 0.0040000000000000001, 'min_samples_leaf': 22, 'n_estimators': 940, 'subsample': 0.39999999999999947, 'min_samples_split': 34, 'max_depth': 2}  \n",
    "Tuned model has the following cross-validated F1 score: 0.808515924705  \n",
    "  \n",
    "Made predictions in 0.0035 seconds.  \n",
    "Tuned model has a training F1 score of 0.8657.  \n",
    "Made predictions in 0.0022 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.7770.  \n",
    "Tuned model has the following parameters: {'loss': 'exponential', 'max_features': 0.78999999999999992, 'learning_rate': 0.012, 'min_samples_leaf': 33, 'n_estimators': 360, 'subsample': 0.83999999999999986, 'min_samples_split': 29, 'max_depth': 16}  \n",
    "Tuned model has the following cross-validated F1 score: 0.816649583558  \n",
    "  \n",
    "Made predictions in 0.0009 seconds.  \n",
    "Tuned model has a training F1 score of 0.8404.  \n",
    "Made predictions in 0.0014 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.7914.  \n",
    "Tuned model has the following parameters: {'loss': 'deviance', 'max_features': 0.27999999999999997, 'learning_rate': 0.13900000000000001, 'min_samples_leaf': 3, 'n_estimators': 80, 'subsample': 0.26999999999999935, 'min_samples_split': 65, 'max_depth': 8}  \n",
    "Tuned model has the following cross-validated F1 score: 0.816718020542  \n",
    "  \n",
    "Made predictions in 0.0011 seconds.  \n",
    "Tuned model has a training F1 score of 0.8299.  \n",
    "Made predictions in 0.0014 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.7626.  \n",
    "Tuned model has the following parameters: {'loss': 'deviance', 'max_features': 0.73999999999999988, 'learning_rate': 0.023, 'min_samples_leaf': 22, 'n_estimators': 80, 'subsample': 0.56999999999999962, 'min_samples_split': 79, 'max_depth': 33}  \n",
    "Tuned model has the following cross-validated F1 score: 0.828836908933  \n",
    "  \n",
    "Made predictions in 0.0009 seconds.  \n",
    "Tuned model has a training F1 score of 0.8833.  \n",
    "Made predictions in 0.0005 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.7660.  \n",
    "Tuned model has the following parameters: {'loss': 'exponential', 'max_features': 0.77999999999999992, 'learning_rate': 0.14100000000000001, 'min_samples_leaf': 3, 'n_estimators': 60, 'subsample': 0.4399999999999995, 'min_samples_split': 82, 'max_depth': 12}  \n",
    "Tuned model has the following cross-validated F1 score: 0.813103986818  \n",
    "  \n",
    "Made predictions in 0.0010 seconds.  \n",
    "Tuned model has a training F1 score of 0.8419.  \n",
    "Made predictions in 0.0006 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.8000.  \n",
    "Tuned model has the following parameters: {'loss': 'deviance', 'max_features': 0.70999999999999996, 'learning_rate': 0.023, 'min_samples_leaf': 15, 'n_estimators': 45, 'subsample': 0.95999999999999996, 'min_samples_split': 82, 'max_depth': 38}  \n",
    "Tuned model has the following cross-validated F1 score: 0.822982843027  \n",
    "  \n",
    "Made predictions in 0.0013 seconds.  \n",
    "Tuned model has a training F1 score of 0.8419.  \n",
    "Made predictions in 0.0014 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.7919.  \n",
    "Tuned model has the following parameters: {'loss': 'deviance', 'max_features': 0.38999999999999996, 'learning_rate': 0.012, 'min_samples_leaf': 4, 'n_estimators': 80, 'subsample': 0.63999999999999968, 'min_samples_split': 63, 'max_depth': 8}  \n",
    "Tuned model has the following cross-validated F1 score: 0.828817284885  \n",
    "  \n",
    "Made predictions in 0.0011 seconds.  \n",
    "Tuned model has a training F1 score of 0.8246.  \n",
    "Made predictions in 0.0008 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.7681.  \n",
    "Tuned model has the following parameters: {'loss': 'deviance', 'max_features': 0.67999999999999994, 'learning_rate': 0.018000000000000002, 'min_samples_leaf': 25, 'n_estimators': 75, 'subsample': 0.70999999999999974, 'min_samples_split': 90, 'max_depth': 22}  \n",
    "Tuned model has the following cross-validated F1 score: 0.825569476809  \n",
    "  \n",
    "Made predictions in 0.0012 seconds.  \n",
    "Tuned model has a training F1 score of 0.8419.  \n",
    "Made predictions in 0.0006 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.8000.  \n",
    "Tuned model has the following parameters: {'loss': 'exponential', 'max_features': 0.54999999999999993, 'learning_rate': 0.012, 'min_samples_leaf': 2, 'n_estimators': 80, 'subsample': 0.91999999999999993, 'min_samples_split': 90, 'max_depth': 29}\n",
    "Tuned model has the following cross-validated F1 score: 0.825525333307  \n",
    "  \n",
    "Made predictions in 0.0012 seconds.  \n",
    "Tuned model has a training F1 score of 0.8354.  \n",
    "Made predictions in 0.0012 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.7947.  \n",
    "Tuned model has the following parameters: {'loss': 'exponential', 'max_features': 0.35999999999999999, 'learning_rate': 0.012, 'min_samples_leaf': 3, 'n_estimators': 85, 'subsample': 0.93999999999999995, 'min_samples_split': 110, 'max_depth': 33}  \n",
    "Tuned model has the following cross-validated F1 score: 0.821714068606  \n",
    "  \n",
    "Made predictions in 0.0013 seconds.  \n",
    "Tuned model has a training F1 score of 0.8390.  \n",
    "Made predictions in 0.0007 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.8000.  \n",
    "Tuned model has the following parameters: {'loss': 'exponential', 'max_features': 0.72999999999999987, 'learning_rate': 0.0090000000000000011, 'min_samples_leaf': 7, 'n_estimators': 90, 'subsample': 0.94999999999999996, 'min_samples_split': 85, 'max_depth': 22}  \n",
    "Tuned model has the following cross-validated F1 score: 0.829450745975  \n",
    "  \n",
    "Made predictions in 0.0010 seconds.  \n",
    "Tuned model has a training F1 score of 0.8326.  \n",
    "Made predictions in 0.0006 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.7626.  \n",
    "Tuned model has the following parameters: {'loss': 'deviance', 'max_features': 0.66999999999999993, 'learning_rate': 0.022000000000000002, 'min_samples_leaf': 29, 'n_estimators': 60, 'subsample': 0.90999999999999992, 'min_samples_split': 119, 'max_depth': 48}  \n",
    "Tuned model has the following cross-validated F1 score: 0.829566487891  \n",
    "  \n",
    "Made predictions in 0.0009 seconds.  \n",
    "Tuned model has a training F1 score of 0.8319.  \n",
    "Made predictions in 0.0005 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.7947.  \n",
    "Tuned model has the following parameters: {'loss': 'deviance', 'max_features': 0.64999999999999991, 'learning_rate': 0.018000000000000002, 'min_samples_leaf': 3, 'n_estimators': 40, 'subsample': 0.74999999999999978, 'min_samples_split': 97, 'max_depth': 28}  \n",
    "Tuned model has the following cross-validated F1 score: 0.833658437641  \n",
    "\n",
    "Made predictions in 0.0011 seconds.  \n",
    "Tuned model has a training F1 score of 0.8372.  \n",
    "Made predictions in 0.0013 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.7947.  \n",
    "Tuned model has the following parameters: {'loss': 'deviance', 'max_features': 0.6399999999999999, 'learning_rate': 0.014999999999999999, 'min_samples_leaf': 7, 'n_estimators': 70, 'subsample': 0.70999999999999974, 'min_samples_split': 107, 'max_depth': 40}  \n",
    "Tuned model has the following cross-validated F1 score: 0.835877410169  \n",
    "\n",
    "Made predictions in 0.0009 seconds.  \n",
    "Tuned model has a training F1 score of 0.8443.  \n",
    "Made predictions in 0.0005 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.8000.  \n",
    "Tuned model has the following parameters: {'loss': 'deviance', 'max_features': 0.88000000000000045, 'learning_rate': 0.014000000000000002, 'min_samples_leaf': 3, 'n_estimators': 75, 'subsample': 0.63999999999999968, 'min_samples_split': 152, 'max_depth': 26}  \n",
    "Tuned model has the following cross-validated F1 score: 0.835877410169  \n",
    "\n",
    "Made predictions in 0.0009 seconds.  \n",
    "Tuned model has a training F1 score of 0.8430.  \n",
    "Made predictions in 0.0006 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.8000.  \n",
    "Tuned model has the following parameters: {'loss': 'exponential', 'max_features': 0.78000000000000036, 'learning_rate': 0.02, 'min_samples_leaf': 7, 'n_estimators': 70, 'subsample': 0.84999999999999987, 'min_samples_split': 172, 'max_depth': 34}  \n",
    "Tuned model has the following cross-validated F1 score: 0.836422914422  \n",
    "\n",
    "Made predictions in 0.0010 seconds.  \n",
    "Tuned model has a training F1 score of 0.8435.  \n",
    "Made predictions in 0.0006 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.7945.  \n",
    "Tuned model has the following parameters: {'loss': 'exponential', 'max_features': 0.98000000000000032, 'learning_rate': 0.017000000000000001, 'min_samples_leaf': 14, 'n_estimators': 80, 'subsample': 0.75999999999999979, 'min_samples_split': 132, 'max_depth': 20}  \n",
    "Tuned model has the following cross-validated F1 score: 0.834673181398  \n",
    "\n",
    "Made predictions in 0.0017 seconds.  \n",
    "Tuned model has a training F1 score of 0.8319.  \n",
    "Made predictions in 0.0008 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.7947.  \n",
    "Tuned model has the following parameters: {'loss': 'deviance', 'max_features': 0.94400000000000028, 'learning_rate': 0.0045000000000000005, 'min_samples_leaf': 3, 'n_estimators': 120, 'subsample': 0.7769999999999998, 'min_samples_split': 66, 'max_depth': 43}  \n",
    "Tuned model has the following cross-validated F1 score: 0.832254221763  \n",
    "\n",
    "Made predictions in 0.0009 seconds.  \n",
    "Tuned model has a training F1 score of 0.8430.  \n",
    "Made predictions in 0.0010 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.7838.  \n",
    "Tuned model has the following parameters: {'loss': 'deviance', 'learning_rate': 0.022799999999999924, 'min_samples_leaf': 9, 'n_estimators': 55, 'subsample': 0.86999999999999988, 'min_weight_fraction_leaf': 0.070000000000000007, 'max_features': 0.85000000000000042, 'min_samples_split': 140, 'max_depth': 25}  \n",
    "Tuned model has the following cross-validated F1 score: 0.833882667564  \n",
    "\n",
    "Made predictions in 0.0010 seconds.  \n",
    "Tuned model has a training F1 score of 0.8383.  \n",
    "Made predictions in 0.0005 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.7947.  \n",
    "Tuned model has the following parameters: {'loss': 'deviance', 'learning_rate': 0.021599999999999932, 'min_samples_leaf': 4, 'n_estimators': 51, 'subsample': 1.0, 'min_weight_fraction_leaf': 0.0060000000000000001, 'max_features': 0.88000000000000045, 'min_samples_split': 203, 'max_depth': 10}  \n",
    "Tuned model has the following cross-validated F1 score: 0.832001853676  \n",
    "\n",
    "Made predictions in 0.0011 seconds.  \n",
    "Tuned model has a training F1 score of 0.8419.  \n",
    "Made predictions in 0.0006 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.7947.  \n",
    "Tuned model has the following parameters: {'loss': 'deviance', 'learning_rate': 0.012799999999999983, 'min_samples_leaf': 9, 'n_estimators': 87, 'subsample': 0.97999999999999998, 'min_weight_fraction_leaf': 0.019, 'max_features': 0.9400000000000005, 'min_samples_split': 189, 'max_depth': 16}  \n",
    "Tuned model has the following cross-validated F1 score: 0.838871894014  \n",
    "\n",
    "Made predictions in 0.0016 seconds.  \n",
    "Tuned model has a training F1 score of 0.8849.  \n",
    "Made predictions in 0.0008 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.7692.  \n",
    "Tuned model has the following parameters: {'loss': 'deviance', 'learning_rate': 0.021499999999999929, 'min_samples_leaf': 4, 'n_estimators': 73, 'subsample': 0.61999999999999966, 'min_weight_fraction_leaf': 0.070000000000000007, 'max_features': 0.8200000000000004, 'min_samples_split': 8, 'max_depth': 13}  \n",
    "Tuned model has the following cross-validated F1 score: 0.83090697268  \n",
    "\n",
    "Made predictions in 0.0009 seconds.  \n",
    "Tuned model has a training F1 score of 0.8394.  \n",
    "Made predictions in 0.0005 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.7919.  \n",
    "Tuned model has the following parameters: {'loss': 'deviance', 'learning_rate': 0.022399999999999927, 'min_samples_leaf': 17, 'n_estimators': 67, 'subsample': 0.85999999999999988, 'min_weight_fraction_leaf': 0.067000000000000004, 'max_features': 0.46000000000000008, 'min_samples_split': 174, 'max_depth': 25}  \n",
    "Tuned model has the following cross-validated F1 score: 0.836170605769  \n",
    "\n",
    "Made predictions in 0.0009 seconds.  \n",
    "Tuned model has a training F1 score of 0.8383.  \n",
    "Made predictions in 0.0009 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.7947.  \n",
    "Tuned model has the following parameters: {'loss': 'deviance', 'learning_rate': 0.015499999999999967, 'min_samples_leaf': 4, 'n_estimators': 71, 'subsample': 0.94999999999999996, 'min_weight_fraction_leaf': 0.0, 'max_features': 0.85000000000000042, 'min_samples_split': 224, 'max_depth': 16}  \n",
    "Tuned model has the following cross-validated F1 score: 0.833365332874  \n",
    "\n",
    "Made predictions in 0.0009 seconds.  \n",
    "Tuned model has a training F1 score of 0.8419.  \n",
    "Made predictions in 0.0005 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.7947.  \n",
    "Tuned model has the following parameters: {'loss': 'exponential', 'max_leaf_nodes': 567, 'learning_rate': 0.015799999999999967, 'min_samples_leaf': 5, 'n_estimators': 91, 'subsample': 0.69999999999999973, 'min_weight_fraction_leaf': 0.0, 'max_features': 0.85000000000000042, 'min_samples_split': 185}  \n",
    "Tuned model has the following cross-validated F1 score: 0.833428430577  \n",
    "\n",
    "Made predictions in 0.0010 seconds.  \n",
    "Tuned model has a training F1 score of 0.8383.  \n",
    "Made predictions in 0.0011 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.7947.  \n",
    "Tuned model has the following parameters: {'loss': 'exponential', 'max_leaf_nodes': 192, 'learning_rate': 0.012499999999999985, 'min_samples_leaf': 2, 'n_estimators': 103, 'subsample': 0.95999999999999996, 'min_weight_fraction_leaf': 0.050000000000000003, 'max_features': 0.64000000000000024, 'min_samples_split': 233}  \n",
    "Tuned model has the following cross-validated F1 score: 0.833365332874  \n",
    "\n",
    "Made predictions in 0.0013 seconds.  \n",
    "Tuned model has a training F1 score of 0.8472.  \n",
    "Made predictions in 0.0012 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.7891.  \n",
    "Tuned model has the following parameters: {'loss': 'exponential', 'max_leaf_nodes': 82, 'learning_rate': 0.015299999999999968, 'min_samples_leaf': 10, 'n_estimators': 95, 'subsample': 0.5499999999999996, 'min_weight_fraction_leaf': 0.0, 'max_features': 0.91000000000000048, 'min_samples_split': 47}  \n",
    "Tuned model has the following cross-validated F1 score: 0.832784082483  \n",
    "\n",
    "Made predictions in 0.0011 seconds.  \n",
    "Tuned model has a training F1 score of 0.8428.  \n",
    "Made predictions in 0.0006 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.7808.  \n",
    "Tuned model has the following parameters: {'loss': 'exponential', 'max_leaf_nodes': 692, 'learning_rate': 0.016799999999999961, 'min_samples_leaf': 7, 'n_estimators': 87, 'subsample': 0.93999999999999995, 'min_weight_fraction_leaf': 0.070000000000000007, 'max_features': 0.88000000000000045, 'min_samples_split': 149}  \n",
    "Tuned model has the following cross-validated F1 score: 0.836796032346  \n",
    "\n",
    "Made predictions in 0.0014 seconds.  \n",
    "Tuned model has a training F1 score of 0.8522.  \n",
    "Made predictions in 0.0016 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.7808.  \n",
    "Tuned model has the following parameters: {'loss': 'exponential', 'max_leaf_nodes': 272, 'learning_rate': 0.010999999999999999, 'min_samples_leaf': 8, 'n_estimators': 100, 'subsample': 0.96999999999999997, 'min_weight_fraction_leaf': 0.02, 'max_features': 0.70000000000000029, 'min_samples_split': 54}  \n",
    "Tuned model has the following cross-validated F1 score: 0.837583519622  \n",
    "\n",
    "Made predictions in 0.0010 seconds.  \n",
    "Tuned model has a training F1 score of 0.8465.  \n",
    "Made predictions in 0.0011 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.7891.  \n",
    "Tuned model has the following parameters: {'loss': 'exponential', 'learning_rate': 0.036999999999999977, 'min_samples_leaf': 4, 'n_estimators': 65, 'subsample': 0.96999999999999997, 'min_weight_fraction_leaf': 0.070000000000000007, 'max_features': 0.61000000000000021, 'min_samples_split': 149}  \n",
    "Tuned model has the following cross-validated F1 score: 0.834382828721  \n",
    "\n",
    "Made predictions in 0.0010 seconds.  \n",
    "Tuned model has a training F1 score of 0.8596.  \n",
    "Made predictions in 0.0014 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.7891.  \n",
    "Tuned model has the following parameters: {'loss': 'exponential', 'learning_rate': 0.01999999999999999, 'min_samples_leaf': 5, 'n_estimators': 65, 'subsample': 0.93999999999999995, 'min_weight_fraction_leaf': 0.02, 'max_features': 0.52000000000000013}  \n",
    "Tuned model has the following cross-validated F1 score: 0.832072288098  \n",
    "\n",
    "Made predictions in 0.0010 seconds.  \n",
    "Tuned model has a training F1 score of 0.8534.  \n",
    "Made predictions in 0.0006 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.7891.  \n",
    "Tuned model has the following parameters: {'loss': 'exponential', 'learning_rate': 0.015999999999999993, 'min_samples_leaf': 1, 'n_estimators': 70, 'subsample': 0.75999999999999979, 'min_weight_fraction_leaf': 0.059999999999999998, 'max_features': 0.8200000000000004}  \n",
    "Tuned model has the following cross-validated F1 score: 0.832042882216  \n",
    "\n",
    "Made predictions in 0.0010 seconds.  \n",
    "Tuned model has a training F1 score of 0.8720.  \n",
    "Made predictions in 0.0005 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.7838.  \n",
    "Tuned model has the following parameters: {'loss': 'exponential', 'learning_rate': 0.025999999999999988, 'min_samples_leaf': 5, 'n_estimators': 60, 'subsample': 0.91999999999999993, 'max_features': 0.91000000000000048}  \n",
    "Tuned model has the following cross-validated F1 score: 0.834492081418  \n",
    "\n",
    "Made predictions in 0.0009 seconds.  \n",
    "Tuned model has a training F1 score of 0.8615.  \n",
    "Made predictions in 0.0005 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.7838.  \n",
    "Tuned model has the following parameters: {'loss': 'exponential', 'learning_rate': 0.025999999999999988, 'min_samples_leaf': 9, 'n_estimators': 50, 'subsample': 0.91999999999999993, 'max_features': 0.59999999999999998}  \n",
    "Tuned model has the following cross-validated F1 score: 0.833350074267  \n",
    "  \n",
    "Made predictions in 0.0012 seconds.  \n",
    "Tuned model has a training F1 score of 0.8432.  \n",
    "Made predictions in 0.0012 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.8026.  \n",
    "Tuned model has the following parameters: {'loss': 'exponential', 'learning_rate': 0.013999999999999997, 'min_samples_leaf': 19, 'n_estimators': 55, 'subsample': 0.91999999999999993, 'max_features': 0.73000000000000009, 'max_depth': 30}  \n",
    "Tuned model has the following cross-validated F1 score: 0.831278179172  \n",
    "  \n",
    "Made predictions in 0.0013 seconds.  \n",
    "Tuned model has a training F1 score of 0.8516.  \n",
    "Made predictions in 0.0013 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.7838.  \n",
    "Tuned model has the following parameters: {'loss': 'exponential', 'learning_rate': 0.011999999999999999, 'min_samples_leaf': 17, 'n_estimators': 69, 'subsample': 0.87999999999999989, 'max_features': 0.88000000000000023, 'max_depth': 58}  \n",
    "Tuned model has the following cross-validated F1 score: 0.832391580549  \n",
    "\n",
    "Made predictions in 0.0010 seconds.  \n",
    "Tuned model has a training F1 score of 0.8371.  \n",
    "Made predictions in 0.0006 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.7801.  \n",
    "Tuned model has the following parameters: {'loss': 'exponential', 'learning_rate': 0.032999999999999981, 'min_samples_leaf': 50, 'n_estimators': 72, 'subsample': 0.93999999999999995, 'max_features': 0.65000000000000002, 'max_depth': 3}  \n",
    "Tuned model has the following cross-validated F1 score: 0.832838429338  \n",
    "  \n",
    "Made predictions in 0.0012 seconds.  \n",
    "Tuned model has a training F1 score of 0.8273.  \n",
    "Made predictions in 0.0013 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.7681.  \n",
    "Tuned model has the following parameters: {'loss': 'deviance', 'learning_rate': 0.020999999999999991, 'min_samples_leaf': 43, 'n_estimators': 83, 'subsample': 0.92999999999999994, 'max_features': 0.84000000000000019, 'max_depth': 13}  \n",
    "Tuned model has the following cross-validated F1 score: 0.831585549197  \n",
    "  \n",
    "Made predictions in 0.0012 seconds.  \n",
    "Tuned model has a training F1 score of 0.8444.  \n",
    "Made predictions in 0.0013 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.7606.  \n",
    "Tuned model has the following parameters: {'loss': 'deviance', 'learning_rate': 0.01999999999999999, 'min_samples_leaf': 26, 'n_estimators': 65, 'subsample': 0.96999999999999997, 'max_features': 0.63, 'max_depth': 46}  \n",
    "Tuned model has the following cross-validated F1 score: 0.829212598335  \n",
    "  \n",
    "Made predictions in 0.0010 seconds.  \n",
    "Tuned model has a training F1 score of 0.8318.  \n",
    "Made predictions in 0.0012 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.7770.  \n",
    "Tuned model has the following parameters: {'loss': 'exponential', 'learning_rate': 0.030999999999999986, 'min_samples_leaf': 49, 'n_estimators': 74, 'subsample': 0.90070000000000061, 'max_features': 0.89000000000000024, 'max_depth': 38}  \n",
    "Tuned model has the following cross-validated F1 score: 0.835407181645  \n",
    "  \n",
    "Made predictions in 0.0012 seconds.  \n",
    "Tuned model has a training F1 score of 0.8246.  \n",
    "Made predictions in 0.0014 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.7681.  \n",
    "Tuned model has the following parameters: {'loss': 'deviance', 'learning_rate': 0.014999999999999996, 'min_samples_leaf': 37, 'n_estimators': 77, 'subsample': 0.95680000000000009, 'max_features': 0.77000000000000013, 'max_depth': 46}  \n",
    "Tuned model has the following cross-validated F1 score: 0.831035189431  \n",
    "  \n",
    "Made predictions in 0.0013 seconds.  \n",
    "Tuned model has a training F1 score of 0.8273.  \n",
    "Made predictions in 0.0014 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.7681.  \n",
    "Tuned model has the following parameters: {'loss': 'deviance', 'learning_rate': 0.015999999999999993, 'min_samples_leaf': 40, 'n_estimators': 93, 'subsample': 0.95350000000000013, 'max_features': 0.95000000000000029, 'max_depth': 40}  \n",
    "Tuned model has the following cross-validated F1 score: 0.829073471903  \n",
    "  \n",
    "Made predictions in 0.0038 seconds.  \n",
    "Tuned model has a training F1 score of 0.8717.  \n",
    "Made predictions in 0.0018 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.7724.  \n",
    "Tuned model has the following parameters: {'loss': 'deviance', 'learning_rate': 0.014999999999999996, 'min_samples_leaf': 20, 'n_estimators': 90, 'subsample': 0.85120000000000107, 'max_features': 0.9600000000000003, 'max_depth': 46}  \n",
    "Tuned model has the following cross-validated F1 score: 0.834536253109  \n",
    "  \n",
    "Made predictions in 0.0012 seconds.  \n",
    "Tuned model has a training F1 score of 0.8246.  \n",
    "Made predictions in 0.0007 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.7681.  \n",
    "Tuned model has the following parameters: {'loss': 'deviance', 'learning_rate': 0.015999999999999993, 'min_samples_leaf': 42, 'n_estimators': 74, 'subsample': 0.90730000000000055, 'max_features': 0.95000000000000029, 'max_depth': 56}  \n",
    "Tuned model has the following cross-validated F1 score: 0.829073471903  \n",
    "  \n",
    "Made predictions in 0.0032 seconds.  \n",
    "Tuned model has a training F1 score of 0.8547.\n",
    "Made predictions in 0.0008 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.7838.  \n",
    "Tuned model has the following parameters: {'n_estimators': 97, 'loss': 'deviance', 'learning_rate': 0.010999999999999999, 'max_depth': 46, 'min_samples_leaf': 25}  \n",
    "Tuned model has the following cross-validated F1 score: 0.834170280423  \n",
    "  \n",
    "Made predictions in 0.0012 seconds.  \n",
    "Tuned model has a training F1 score of 0.8326.  \n",
    "Made predictions in 0.0026 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.7681.  \n",
    "Tuned model has the following parameters: {'n_estimators': 93, 'loss': 'exponential', 'learning_rate': 0.017999999999999995, 'max_depth': 58, 'min_samples_leaf': 43}  \n",
    "Tuned model has the following cross-validated F1 score: 0.828349957691  \n",
    "  \n",
    "Made predictions in 0.0018 seconds.  \n",
    "Tuned model has a training F1 score of 0.8621.  \n",
    "Made predictions in 0.0007 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.7945.  \n",
    "Tuned model has the following parameters: {'n_estimators': 69, 'loss': 'exponential', 'learning_rate': 0.016999999999999994, 'max_depth': 44, 'min_samples_leaf': 24}  \n",
    "Tuned model has the following cross-validated F1 score: 0.834167559335"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Alternative Model Tuning - KNeighborsClassifier</h3>\n",
    "Additionally, I've ran the KNeighbors classification also with a RandomizedSearchCV, and it seems that F<sub>1</sub> for testing score is consistently above 0.8. (code and trial results follow).  This is a fairly strong indication that, for this classification task, KNeighbors is superior to the GradientBoostingClassifier discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n",
      "Made predictions in 0.0116 seconds.\n",
      "Tuned model has a training F1 score of 1.0000.\n",
      "Made predictions in 0.0048 seconds.\n",
      "Tuned model has a testing F1 score of 0.8077.\n",
      "Tuned model has the following parameters: {'n_neighbors': 21, 'metric': 'manhattan', 'weights': 'distance', 'leaf_size': 65, 'algorithm': 'ball_tree'}\n",
      "Tuned model had cross-validated F1 score of: 0.813113482211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done 1000 out of 1000 | elapsed:    5.3s finished\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create the parameters list you wish to tune\n",
    "kn_parameters = {\n",
    "    \"n_neighbors\": range(1,100,1)\n",
    "    ,\"algorithm\":[\"kd_tree\",\"ball_tree\"]\n",
    "    ,\"leaf_size\": range(1,100,1)\n",
    "    ,\"metric\":[\"manhattan\"]\n",
    "    ,\"weights\":[\"distance\"]\n",
    "}\n",
    "\n",
    "# TODO: Initialize the classifier\n",
    "kn_clf = KNeighborsClassifier()\n",
    "\n",
    "# TODO: Make an f1 scoring function using 'make_scorer' \n",
    "f1_scorer = make_scorer(f1_score, pos_label=\"yes\")\n",
    "\n",
    "# TODO: Perform grid search on the classifier using the f1_scorer as the scoring method\n",
    "kn_grid_obj = RandomizedSearchCV(kn_clf, kn_parameters, n_iter=100, scoring=f1_scorer, verbose=1, n_jobs=1, cv=10)\n",
    "\n",
    "# TODO: Fit the grid search object to the training data and find the optimal parameters\n",
    "kn_grid_obj = kn_grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Get the estimator\n",
    "kn_clf = kn_grid_obj.best_estimator_\n",
    "\n",
    "# Report the final F1 score for training and testing after parameter tuning\n",
    "print(\"Tuned model has a training F1 score of {:.4f}.\".format(predict_labels(kn_clf, X_train, y_train)))\n",
    "print(\"Tuned model has a testing F1 score of {:.4f}.\".format(predict_labels(kn_clf, X_test, y_test)))\n",
    "\n",
    "# Report the parameters of the best estimator\n",
    "print(\"Tuned model has the following parameters: \" + str(kn_grid_obj.best_params_))\n",
    "print(\"Tuned model had cross-validated F1 score of: \" + str(kn_grid_obj.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>K-Nearest Neighbors Trial Results</h3>\n",
    "<h4>cv = 10</h4>\n",
    "\n",
    "Made predictions in 0.0206 seconds.  \n",
    "Tuned model has a training F1 score of 1.0000.  \n",
    "Made predictions in 0.0073 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.8025.  \n",
    "Tuned model has the following parameters: {'leaf_size': 37, 'weights': 'distance', 'metric': 'manhattan', 'n_neighbors': 28, 'algorithm': 'kd_tree'}  \n",
    "Tuned model had cross-validated F1 score of: 0.808156299841  \n",
    "  \n",
    "Made predictions in 0.1427 seconds.  \n",
    "Tuned model has a training F1 score of 1.0000.  \n",
    "Made predictions in 0.0462 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.8235.  \n",
    "Tuned model has the following parameters: {'leaf_size': 1, 'weights': 'distance', 'metric': 'manhattan', 'n_neighbors': 18, 'algorithm': 'kd_tree'}  \n",
    "Tuned model had cross-validated F1 score of: 0.81092319151  \n",
    "  \n",
    "Made predictions in 0.0512 seconds.  \n",
    "Tuned model has a training F1 score of 1.0000.  \n",
    "Made predictions in 0.0166 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.8101.  \n",
    "Tuned model has the following parameters: {'leaf_size': 3, 'weights': 'distance', 'metric': 'manhattan', 'n_neighbors': 40, 'algorithm': 'kd_tree'}  \n",
    "Tuned model had cross-validated F1 score of: 0.809941706117  \n",
    "  \n",
    "Made predictions in 0.0219 seconds.  \n",
    "Tuned model has a training F1 score of 1.0000.  \n",
    "Made predictions in 0.0070 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.8235.  \n",
    "Tuned model has the following parameters: {'leaf_size': 14, 'weights': 'distance', 'metric': 'manhattan', 'n_neighbors': 18, 'algorithm': 'kd_tree'}  \n",
    "Tuned model had cross-validated F1 score of: 0.812542239129  \n",
    "  \n",
    "Made predictions in 0.1442 seconds.  \n",
    "Tuned model has a training F1 score of 1.0000.  \n",
    "Made predictions in 0.0456 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.8235.  \n",
    "Tuned model has the following parameters: {'leaf_size': 1, 'weights': 'distance', 'metric': 'manhattan', 'n_neighbors': 18, 'algorithm': 'kd_tree'}  \n",
    "Tuned model had cross-validated F1 score of: 0.81092319151  \n",
    "  \n",
    "Made predictions in 0.0502 seconds.  \n",
    "Tuned model has a training F1 score of 1.0000.  \n",
    "Made predictions in 0.0168 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.8235.  \n",
    "Tuned model has the following parameters: {'leaf_size': 3, 'weights': 'distance', 'metric': 'manhattan', 'n_neighbors': 18, 'algorithm': 'kd_tree'}  \n",
    "Tuned model had cross-validated F1 score of: 0.81105363642  \n",
    "  \n",
    "Made predictions in 0.0142 seconds.  \n",
    "Tuned model has a training F1 score of 1.0000.  \n",
    "Made predictions in 0.0048 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.8235.  \n",
    "Tuned model has the following parameters: {'leaf_size': 40, 'weights': 'distance', 'metric': 'manhattan', 'n_neighbors': 18, 'algorithm': 'kd_tree'}  \n",
    "Tuned model had cross-validated F1 score of: 0.809987812423  \n",
    "  \n",
    "Made predictions in 0.0155 seconds.  \n",
    "Tuned model has a training F1 score of 1.0000.  \n",
    "Made predictions in 0.0062 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.8235.  \n",
    "Tuned model has the following parameters: {'leaf_size': 25, 'weights': 'distance', 'metric': 'manhattan', 'n_neighbors': 18, 'algorithm': 'kd_tree'}  \n",
    "Tuned model had cross-validated F1 score of: 0.810851114041  \n",
    "  \n",
    "Made predictions in 0.0135 seconds.  \n",
    "Tuned model has a training F1 score of 1.0000.  \n",
    "Made predictions in 0.0048 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.8235.  \n",
    "Tuned model has the following parameters: {'weights': 'distance', 'leaf_size': 61, 'algorithm': 'kd_tree', 'n_neighbors': 18, 'metric': 'manhattan'}  \n",
    "Tuned model had cross-validated F1 score of: 0.809987812423  \n",
    "  \n",
    "<h4>cv=20</h4>\n",
    "  \n",
    "Made predictions in 0.0123 seconds.  \n",
    "Tuned model has a training F1 score of 1.0000.  \n",
    "Made predictions in 0.0052 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.8182.  \n",
    "Tuned model has the following parameters: {'weights': 'distance', 'leaf_size': 91, 'algorithm': 'kd_tree', 'n_neighbors': 20, 'metric': 'manhattan'}  \n",
    "Tuned model had cross-validated F1 score of: 0.814326355341  \n",
    "  \n",
    "Made predictions in 0.0161 seconds.  \n",
    "Tuned model has a training F1 score of 1.0000.  \n",
    "Made predictions in 0.0062 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.8289.  \n",
    "Tuned model has the following parameters: {'weights': 'distance', 'algorithm': 'kd_tree', 'n_neighbors': 14, 'metric': 'manhattan'}  \n",
    "Tuned model had cross-validated F1 score of: 0.813482589874  \n",
    "  \n",
    "Made predictions in 0.0164 seconds.  \n",
    "Tuned model has a training F1 score of 1.0000.  \n",
    "Made predictions in 0.0056 seconds.  \n",
    "Tuned model has a **testing F1 score** of 0.8289.  \n",
    "Tuned model has the following parameters: {'weights': 'distance', 'algorithm': 'kd_tree', 'n_neighbors': 14, 'metric': 'manhattan'}  \n",
    "Tuned model had cross-validated F1 score of: 0.813482589874  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: Once you have completed all of the code implementations and successfully answered each question above, you may finalize your work by exporting the iPython Notebook as an HTML document. You can do this by using the menu above and navigating to  \n",
    "**File -> Download as -> HTML (.html)**. Include the finished document along with this notebook as your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python2",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
