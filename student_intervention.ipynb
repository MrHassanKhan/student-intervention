{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "## Supervised Learning\n",
    "## Project 2: Building a Student Intervention System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the second project of the Machine Learning Engineer Nanodegree! In this notebook, some template code has already been provided for you, and it will be your job to implement the additional functionality necessary to successfully complete this project. Sections that begin with **'Implementation'** in the header indicate that the following block of code will require additional functionality which you must provide. Instructions will be provided for each section and the specifics of the implementation are marked in the code block with a `'TODO'` statement. Please be sure to read the instructions carefully!\n",
    "\n",
    "In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a **'Question X'** header. Carefully read each question and provide thorough answers in the following text boxes that begin with **'Answer:'**. Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide.  \n",
    "\n",
    ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut. In addition, Markdown cells can be edited by typically double-clicking the cell to enter edit mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 - Classification vs. Regression\n",
    "*Your goal for this project is to identify students who might need early intervention before they fail to graduate. Which type of supervised learning problem is this, classification or regression? Why?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This supervised learning problem is one of **classification**, since the desired output is whether (in a binary sense) a student needs early intervention.  \n",
    "This supervised learning problem does not seek to quantify or convey the strength of the need for intervention, which would be a regression, but instead merely flags (or fails to flag) a student as needing early intervention, under the presumption that the school's administration then intervenes commensurate with the student's need.  \n",
    "In machine learning parlance, the desired output here is a **label**, either yes or no, on the **class** of \"Does this student need early intervention\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Data\n",
    "Run the code cell below to load necessary Python libraries and load the student data. Note that the last column from this dataset, `'passed'`, will be our target label (whether the student graduated or didn't graduate). All other columns are features about each student."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student data read successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Read student data\n",
    "student_data = pd.read_csv(\"student-data.csv\")\n",
    "print \"Student data read successfully!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>address</th>\n",
       "      <th>famsize</th>\n",
       "      <th>Pstatus</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>Mjob</th>\n",
       "      <th>Fjob</th>\n",
       "      <th>...</th>\n",
       "      <th>internet</th>\n",
       "      <th>romantic</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>Dalc</th>\n",
       "      <th>Walc</th>\n",
       "      <th>health</th>\n",
       "      <th>absences</th>\n",
       "      <th>passed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>18</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>at_home</td>\n",
       "      <td>teacher</td>\n",
       "      <td>...</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>17</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>at_home</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>15</td>\n",
       "      <td>U</td>\n",
       "      <td>LE3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>at_home</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>15</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>health</td>\n",
       "      <td>services</td>\n",
       "      <td>...</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>16</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  school sex  age address famsize Pstatus  Medu  Fedu     Mjob      Fjob  \\\n",
       "0     GP   F   18       U     GT3       A     4     4  at_home   teacher   \n",
       "1     GP   F   17       U     GT3       T     1     1  at_home     other   \n",
       "2     GP   F   15       U     LE3       T     1     1  at_home     other   \n",
       "3     GP   F   15       U     GT3       T     4     2   health  services   \n",
       "4     GP   F   16       U     GT3       T     3     3    other     other   \n",
       "\n",
       "   ...   internet romantic  famrel  freetime  goout Dalc Walc health absences  \\\n",
       "0  ...         no       no       4         3      4    1    1      3        6   \n",
       "1  ...        yes       no       5         3      3    1    1      3        4   \n",
       "2  ...        yes       no       4         3      2    2    3      3       10   \n",
       "3  ...        yes      yes       3         2      2    1    1      5        2   \n",
       "4  ...         no       no       4         3      2    1    2      5        4   \n",
       "\n",
       "  passed  \n",
       "0     no  \n",
       "1     no  \n",
       "2    yes  \n",
       "3    yes  \n",
       "4    yes  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_data.head()\n",
    "# Get an idea of the shape of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>traveltime</th>\n",
       "      <th>studytime</th>\n",
       "      <th>failures</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>Dalc</th>\n",
       "      <th>Walc</th>\n",
       "      <th>health</th>\n",
       "      <th>absences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>16.696203</td>\n",
       "      <td>2.749367</td>\n",
       "      <td>2.521519</td>\n",
       "      <td>1.448101</td>\n",
       "      <td>2.035443</td>\n",
       "      <td>0.334177</td>\n",
       "      <td>3.944304</td>\n",
       "      <td>3.235443</td>\n",
       "      <td>3.108861</td>\n",
       "      <td>1.481013</td>\n",
       "      <td>2.291139</td>\n",
       "      <td>3.554430</td>\n",
       "      <td>5.708861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.276043</td>\n",
       "      <td>1.094735</td>\n",
       "      <td>1.088201</td>\n",
       "      <td>0.697505</td>\n",
       "      <td>0.839240</td>\n",
       "      <td>0.743651</td>\n",
       "      <td>0.896659</td>\n",
       "      <td>0.998862</td>\n",
       "      <td>1.113278</td>\n",
       "      <td>0.890741</td>\n",
       "      <td>1.287897</td>\n",
       "      <td>1.390303</td>\n",
       "      <td>8.003096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>22.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>75.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              age        Medu        Fedu  traveltime   studytime    failures  \\\n",
       "count  395.000000  395.000000  395.000000  395.000000  395.000000  395.000000   \n",
       "mean    16.696203    2.749367    2.521519    1.448101    2.035443    0.334177   \n",
       "std      1.276043    1.094735    1.088201    0.697505    0.839240    0.743651   \n",
       "min     15.000000    0.000000    0.000000    1.000000    1.000000    0.000000   \n",
       "25%     16.000000    2.000000    2.000000    1.000000    1.000000    0.000000   \n",
       "50%     17.000000    3.000000    2.000000    1.000000    2.000000    0.000000   \n",
       "75%     18.000000    4.000000    3.000000    2.000000    2.000000    0.000000   \n",
       "max     22.000000    4.000000    4.000000    4.000000    4.000000    3.000000   \n",
       "\n",
       "           famrel    freetime       goout        Dalc        Walc      health  \\\n",
       "count  395.000000  395.000000  395.000000  395.000000  395.000000  395.000000   \n",
       "mean     3.944304    3.235443    3.108861    1.481013    2.291139    3.554430   \n",
       "std      0.896659    0.998862    1.113278    0.890741    1.287897    1.390303   \n",
       "min      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "25%      4.000000    3.000000    2.000000    1.000000    1.000000    3.000000   \n",
       "50%      4.000000    3.000000    3.000000    1.000000    2.000000    4.000000   \n",
       "75%      5.000000    4.000000    4.000000    2.000000    3.000000    5.000000   \n",
       "max      5.000000    5.000000    5.000000    5.000000    5.000000    5.000000   \n",
       "\n",
       "         absences  \n",
       "count  395.000000  \n",
       "mean     5.708861  \n",
       "std      8.003096  \n",
       "min      0.000000  \n",
       "25%      0.000000  \n",
       "50%      4.000000  \n",
       "75%      8.000000  \n",
       "max     75.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_data.describe()\n",
    "# Another view of the data, wholly for my own personal benefit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "school        0\n",
       "sex           0\n",
       "age           0\n",
       "address       0\n",
       "famsize       0\n",
       "Pstatus       0\n",
       "Medu          0\n",
       "Fedu          0\n",
       "Mjob          0\n",
       "Fjob          0\n",
       "reason        0\n",
       "guardian      0\n",
       "traveltime    0\n",
       "studytime     0\n",
       "failures      0\n",
       "schoolsup     0\n",
       "famsup        0\n",
       "paid          0\n",
       "activities    0\n",
       "nursery       0\n",
       "higher        0\n",
       "internet      0\n",
       "romantic      0\n",
       "famrel        0\n",
       "freetime      0\n",
       "goout         0\n",
       "Dalc          0\n",
       "Walc          0\n",
       "health        0\n",
       "absences      0\n",
       "passed        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_data.isnull().apply(sum)\n",
    "# Check whether the dataset is missing information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation: Data Exploration\n",
    "Let's begin by investigating the dataset to determine how many students we have information on, and learn about the graduation rate among these students. In the code cell below, you will need to compute the following:\n",
    "- The total number of students, `n_students`.\n",
    "- The total number of features for each student, `n_features`.\n",
    "- The number of those students who passed, `n_passed`.\n",
    "- The number of those students who failed, `n_failed`.\n",
    "- The graduation rate of the class, `grad_rate`, in percent (%).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of students: 395\n",
      "Number of features: 30\n",
      "Number of students who passed: 265\n",
      "Number of students who failed: 130\n",
      "Graduation rate of the class: 0.67%\n"
     ]
    }
   ],
   "source": [
    "# TODO: Calculate number of students\n",
    "n_students = len(student_data)\n",
    "\n",
    "# TODO: Calculate number of features\n",
    "n_features = len(student_data.columns) - 1\n",
    "    # We are told in the prompt that one and only one column is a label,\n",
    "    # hence this must be subtracted from the count.\n",
    "\n",
    "# TODO: Calculate passing students\n",
    "n_passed = sum(student_data[\"passed\"]==\"yes\")\n",
    "\n",
    "# TODO: Calculate failing students\n",
    "n_failed = sum(student_data[\"passed\"]==\"no\")\n",
    "\n",
    "# TODO: Calculate graduation rate\n",
    "grad_rate = float(n_passed) / n_students\n",
    "    # Python 2.7 defaults to integer division.\n",
    "\n",
    "# Print the results\n",
    "print \"Total number of students: {}\".format(n_students)\n",
    "print \"Number of features: {}\".format(n_features)\n",
    "print \"Number of students who passed: {}\".format(n_passed)\n",
    "print \"Number of students who failed: {}\".format(n_failed)\n",
    "print \"Graduation rate of the class: {:.2f}%\".format(grad_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "In this section, we will prepare the data for modeling, training and testing.\n",
    "\n",
    "### Identify feature and target columns\n",
    "It is often the case that the data you obtain contains non-numeric features. This can be a problem, as most machine learning algorithms expect numeric data to perform computations with.\n",
    "\n",
    "Run the code cell below to separate the student data into feature and target columns to see if any features are non-numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature columns:\n",
      "['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu', 'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences']\n",
      "\n",
      "Target column: passed\n",
      "\n",
      "Feature values:\n",
      "  school sex  age address famsize Pstatus  Medu  Fedu     Mjob      Fjob  \\\n",
      "0     GP   F   18       U     GT3       A     4     4  at_home   teacher   \n",
      "1     GP   F   17       U     GT3       T     1     1  at_home     other   \n",
      "2     GP   F   15       U     LE3       T     1     1  at_home     other   \n",
      "3     GP   F   15       U     GT3       T     4     2   health  services   \n",
      "4     GP   F   16       U     GT3       T     3     3    other     other   \n",
      "\n",
      "    ...    higher internet  romantic  famrel  freetime goout Dalc Walc health  \\\n",
      "0   ...       yes       no        no       4         3     4    1    1      3   \n",
      "1   ...       yes      yes        no       5         3     3    1    1      3   \n",
      "2   ...       yes      yes        no       4         3     2    2    3      3   \n",
      "3   ...       yes      yes       yes       3         2     2    1    1      5   \n",
      "4   ...       yes       no        no       4         3     2    1    2      5   \n",
      "\n",
      "  absences  \n",
      "0        6  \n",
      "1        4  \n",
      "2       10  \n",
      "3        2  \n",
      "4        4  \n",
      "\n",
      "[5 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "# Extract feature columns\n",
    "feature_cols = list(student_data.columns[:-1])\n",
    "\n",
    "# Extract target column 'passed'\n",
    "target_col = student_data.columns[-1] \n",
    "\n",
    "# Show the list of columns\n",
    "print \"Feature columns:\\n{}\".format(feature_cols)\n",
    "print \"\\nTarget column: {}\".format(target_col)\n",
    "\n",
    "# Separate the data into feature data and target data (X_all and y_all, respectively)\n",
    "X_all = student_data[feature_cols]\n",
    "y_all = student_data[target_col]\n",
    "\n",
    "# Show the feature information by printing the first five rows\n",
    "print \"\\nFeature values:\"\n",
    "print X_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Feature Columns\n",
    "\n",
    "As you can see, there are several non-numeric columns that need to be converted! Many of them are simply `yes`/`no`, e.g. `internet`. These can be reasonably converted into `1`/`0` (binary) values.\n",
    "\n",
    "Other columns, like `Mjob` and `Fjob`, have more than two values, and are known as _categorical variables_. The recommended way to handle such a column is to create as many columns as possible values (e.g. `Fjob_teacher`, `Fjob_other`, `Fjob_services`, etc.), and assign a `1` to one of them and `0` to all others.\n",
    "\n",
    "These generated columns are sometimes called _dummy variables_, and we will use the [`pandas.get_dummies()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html?highlight=get_dummies#pandas.get_dummies) function to perform this transformation. Run the code cell below to perform the preprocessing routine discussed in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed feature columns (48 total features):\n",
      "['school_GP', 'school_MS', 'sex_F', 'sex_M', 'age', 'address_R', 'address_U', 'famsize_GT3', 'famsize_LE3', 'Pstatus_A', 'Pstatus_T', 'Medu', 'Fedu', 'Mjob_at_home', 'Mjob_health', 'Mjob_other', 'Mjob_services', 'Mjob_teacher', 'Fjob_at_home', 'Fjob_health', 'Fjob_other', 'Fjob_services', 'Fjob_teacher', 'reason_course', 'reason_home', 'reason_other', 'reason_reputation', 'guardian_father', 'guardian_mother', 'guardian_other', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences']\n"
     ]
    }
   ],
   "source": [
    "def preprocess_features(X):\n",
    "    ''' Preprocesses the student data and converts non-numeric binary variables into\n",
    "        binary (0/1) variables. Converts categorical variables into dummy variables. '''\n",
    "    \n",
    "    # Initialize new output DataFrame\n",
    "    output = pd.DataFrame(index = X.index)\n",
    "\n",
    "    # Investigate each feature column for the data\n",
    "    for col, col_data in X.iteritems():\n",
    "        \n",
    "        # If data type is non-numeric, replace all yes/no values with 1/0\n",
    "        if col_data.dtype == object:\n",
    "            col_data = col_data.replace(['yes', 'no'], [1, 0])\n",
    "\n",
    "        # If data type is categorical, convert to dummy variables\n",
    "        if col_data.dtype == object:\n",
    "            # Example: 'school' => 'school_GP' and 'school_MS'\n",
    "            col_data = pd.get_dummies(col_data, prefix = col)  \n",
    "        \n",
    "        # Collect the revised columns\n",
    "        output = output.join(col_data)\n",
    "    \n",
    "    return output\n",
    "\n",
    "X_all = preprocess_features(X_all)\n",
    "print \"Processed feature columns ({} total features):\\n{}\".format(len(X_all.columns), list(X_all.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation: Training and Testing Data Split\n",
    "So far, we have converted all _categorical_ features into numeric values. For the next step, we split the data (both features and corresponding labels) into training and test sets. In the following code cell below, you will need to implement the following:\n",
    "- Randomly shuffle and split the data (`X_all`, `y_all`) into training and testing subsets.\n",
    "  - Use 300 training points (approximately 75%) and 95 testing points (approximately 25%).\n",
    "  - Set a `random_state` for the function(s) you use, if provided.\n",
    "  - Store the results in `X_train`, `X_test`, `y_train`, and `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 300 samples.\n",
      "Testing set has 95 samples.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Import any additional functionality you may need here\n",
    "\n",
    "# TODO: Set the number of training points\n",
    "num_train = 300\n",
    "\n",
    "# Set the number of testing points\n",
    "num_test = X_all.shape[0] - num_train\n",
    "\n",
    "# TODO: Shuffle and split the dataset into the number of training and testing points above\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_all, y_all, train_size = num_train, test_size = num_test, random_state = 42)\n",
    "\n",
    "# Show the results of the split\n",
    "print \"Training set has {} samples.\".format(X_train.shape[0])\n",
    "print \"Testing set has {} samples.\".format(X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluating Models\n",
    "In this section, you will choose 3 supervised learning models that are appropriate for this problem and available in `scikit-learn`. You will first discuss the reasoning behind choosing these three models by considering what you know about the data and each model's strengths and weaknesses. You will then fit the model to varying sizes of training data (100 data points, 200 data points, and 300 data points) and measure the F<sub>1</sub> score. You will need to produce three tables (one for each model) that shows the training set size, training time, prediction time, F<sub>1</sub> score on the training set, and F<sub>1</sub> score on the testing set.\n",
    "\n",
    "**The following supervised learning models are currently available in** [`scikit-learn`](http://scikit-learn.org/stable/supervised_learning.html) **that you may choose from:**\n",
    "- Gaussian Naive Bayes (GaussianNB)\n",
    "- Decision Trees\n",
    "- Ensemble Methods (Bagging, AdaBoost, Random Forest, Gradient Boosting)\n",
    "- K-Nearest Neighbors (KNeighbors)\n",
    "- Stochastic Gradient Descent (SGDC)\n",
    "- Support Vector Machines (SVM)\n",
    "- Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 - Model Application\n",
    "*List three supervised learning models that are appropriate for this problem. For each model chosen*\n",
    "- Describe one real-world application in industry where the model can be applied. *(You may need to do a small bit of research for this â€” give references!)* \n",
    "- What are the strengths of the model; when does it perform well? \n",
    "- What are the weaknesses of the model; when does it perform poorly?\n",
    "- What makes this model a good candidate for the problem, given what you know about the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three supervised learning models that are appropriate for this problem are **Decision Trees**, **Gradient Boosting**, and **K-Nearest Neighbors**, discussed below.\n",
    "<h3><font color=\"darkgreen\">Decision Trees</font></h3>\n",
    "**Decision Trees** attempt to classify the data by playing 20 questions.  This algorithm works best when there are easily-identified clefts along a feature, and the features are fairly independent of each other.  This allows each question to generate maximal information gain, and reduces having to ask the same or a similar question again later.\n",
    "<h4>Uses</h4>\n",
    "Probably the most impressive collection of scientific and industrial applications of Decision Trees is found in [this 1995 write-up](http://www.cbcb.umd.edu/~salzberg/docs/murthy_thesis/survey/node32.html) by big-data rockstar Sreerama K. Murthy, which includes applications in the fields of astronomy, manufacturing, pharmacology, and medicine.  One particular such application was to [classify objects](http://iopscience.iop.org/article/10.1086/133551/meta) in Hubble Space Telescope images as \"stars, galaxies, cosmic rays, plate defects, and other types of objects ... with over 95% accuracy using data from a single, upaired image\".\n",
    "<h4>Strengths</h4>\n",
    "Decision Trees are conceptually appealing; if we have a binary classification for which there are a host of necessary and sufficient conditions, we can concisely represent this classification exercise as a decision tree.  \n",
    "Similarly, we can represent arbitrary complexity with decision trees (although we may not always want to do so).  Finally, it's possible to tune a Decision Tree to the \"right\" degree of complexity for the dataset.\n",
    "<h4>Weaknesses</h4>\n",
    "Fundamentally, a decision tree is not a good solution when the label boundaries are not feature-axis-aligned.  That is, if y = x is the best classification boundary, a decision tree would need many tiny \"stairs\" to capture that fact.  This would result in a perfectly-functional decision tree, but which requires traversing a computationally large number of nodes to arrive at the correct classification.  \n",
    "Further, the strength of being able to tune a Decision Tree to the \"right\" degree of complexity is, in the same breath, a weakness that allows for a Decision Tree to be improperly tuned, either leading to overgeneralizations that miss most of the training data or to hyper-specializations that overfit at the expense of predictive power.  \n",
    "<h4>Application to Student Intervention</h4>\n",
    "Here, I would presume that certain features included with this dataset have meaningful classification clefts.  For example, if you've missed every day of school, I feel comfortable predicting that you need intervention to pass.  Similarly, if your weekday alcohol consumption is high, that probably means that intervention is needed to pass.  Other intuitive-feeling clefts would be low weekly study time without being a genious, very bad family relationships, and not wanting to take higher education.  \n",
    "Now, the decision tree algorithm doesn't take _my_ input for what are useful features along which to cleave, but I do think those axis-aligned split points exist, and so I would guess that the decision tree algorithm is a good classifier to train on this dataset.\n",
    "\n",
    "<h3><font color=\"darkgreen\">Gradient Boosting</font></h3>\n",
    "The **Gradient Boosting** algorithm tries to learn from what it gets wrong, by iteratively doing slightly better than random chance at guessing on the leftover error.  That doesn't sound terribly impressive, but in practice it has been quite useful.  \n",
    "This algorithm is very similar to the AdaBoost algorithm discussed in lecture, with a technical difference in _how_ it focuses on leftover error (i.e. gradient boost employs gradient descent on the sum of squared residual errors, while AdaBoost tweaks the \"importance\" of datapoints based on the correctness of the previous classification). \n",
    "<h4>Uses</h4>\n",
    "Gradient Boosting has been used successfully for neurorobotics applications, specifically in mapping [EMG](https://en.wikipedia.org/wiki/Electromyography) signals to [a robotic arm](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3885826/) by watching the human participant and measuring brain activity.  Gradient Boosting is also used extensively in facial recognition, whether in [realtime](http://accord-framework.net/docs/html/T_Accord_Vision_Detection_HaarObjectDetector.htm), or to catalog who was present in a [video-steam](http://link.springer.com/chapter/10.1007%2F978-3-642-23123-0_14).  Another visual application of the Gradient Boosting approach is to [detect crop blight](http://imperialjournals.com/index.php/IJIR/article/view/465/449).\n",
    "<h4>Strengths</h4>\n",
    "Being an ensemble method, Gradient Boosting is versatile in that it can use any of a wide variety of base classifiers, from the common decision tree to the inscrutible neural network.  \n",
    "As suggested by the above applications, Gradient Boosting with decision trees has the advantage of being able to work well without the manual injection of domain knowledge.  The base classifier can of course still be poorly-suited to the classification task at hand, but gradient boosting typically makes the best of whatever classifier it's given.  \n",
    "Gradient Boosting is also the algorithm [_du jour_](https://github.com/dmlc/xgboost/tree/master/demo#machine-learning-challenge-winning-solutions) for Kaggle.  That's not a rigorous examination of the merits of the algorithm, but it does speak to the machine learning community's adoption of it, and thus to the likelihood of (1) finding a colleague who understands the algorithm, of (2) finding a robust and well-tested software implementation of the algorithm, and of (3) future practical and theoretical work being done on this algorithm.\n",
    "<h4>Weaknesses</h4>\n",
    "Gradient Boosting is not magic; it is still subject to the constraints of garbage-in:garbage-out.  That is, it requires a weak classifier even to work, and this is not an easy requirement to meet.\n",
    "The algorithm also [doesn't](http://www.cs.columbia.edu/~rocco/Public/mlj9.pdf) handle random noise well.  That is, boosting algorithms tend to kind of obsess over data that it's not correctly classifying, without any mechanism to ignore any one label as erroneous.  \n",
    "This algorithm is also memory-intensive.  This is not surprising, since it has to remember at least some of perhaps thousands of individual weak classifiers to comprise the composite classifier that is the goal of boosting.  While it's a truism that memory is cheap, it's also the case that we will find increasingly larger problems to keep pace with the advances in memory capacity and access speed.\n",
    "<h4>Application to Student Intervention</h4>\n",
    "Here, the idea is that individual, say, Decision Tree classifiers, will be sufficiently good to be classified as \"weak\" on the student intervention dataset.  According to the algorithmic requirements of gradient boosting, that's all that's needed.  The assumption is of course that the Gradient Boosting algorithm will appropriately stitch together individual Decision Tree classifiers to minimize the sum of squared prediction errors, and in so doing, produce a composite classifier with a high degree of accuracy on the test set.\n",
    "\n",
    "<h3><font color=\"darkgreen\">K-Nearest Neighbors</font></h3>\n",
    "**K-Nearest Neighbors** attempts to provide a classification label based off of some measure of **similarity**.  \n",
    "That is, if something has feathers, quacks, and really loves bread, it's probably a duck, even if that's a bit of an oversimplification.\n",
    "<h4>Uses</h4>\n",
    "KNeighbors has been used successfully in [medical research](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2243774/) and [credit card fraud detection](https://www.researchgate.net/publication/236962626_credit_card_fraud_detection_using_anti_k-nearest_algorithm), but could reasonably be expected to produce good results whenever there is a diffusion of datapoints in a particular feature space (read: enough data), within some contiguous space-saving boundary (that is, not highly intermixed).\n",
    "<h4>Strengths</h4>\n",
    "Because K-Nearest Neighbors depends merely on some measure of similarity, it is possible for a KNeighbors classifier to accurately capture arbitrary structural complexity within the data, without the need to inject that domain knowledge in the form of a kernel trick, polynomial degree, or some other function.  \n",
    "This allows a KNeighbors algorithm to merely \"discover\" structure in the data, once a measure of similarity is supplied.  \n",
    "When datapoints which are similar in the featurespace are also similar in the labelspace, KNeighbors is an excellent algorithm.\n",
    "<h4>Weaknesses</h4>\n",
    "KNeighbors has difficulty with edge cases, such as whether someone with really high cholesterol who exercises regularly and eats well would be at risk for heart disease, or whether a particular entry in the [iris dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set#/media/File:Iris_dataset_scatterplot.svg) with median sepal length and sepal width is Iris virginica or Iris versicolor, or whether someone who is socially conservative but supports broad spending on social programs is a \"Republican\" or \"Democrat\".  \n",
    "Further, KNeighbors suffers from the **curse of dimensionality**, where the number of datapoints needed to properly train a KNeighbors algorithm grows exponentially with the number of features (dimensions) given to the classifier.  That is, KNeighbors is really only feasible with less than 10 input features, unless the number of input data is billions upon billions.  \n",
    "Here, we have 300 training points.  That suggests we can probably only hope to use 5 features max, and those may need to be combined / projected featurespaces.\n",
    "<h4>Application to Student Intervention</h4>\n",
    "Here, the assumption is that there is some similarity measure between students who are numerically close to each other in some representation of a feature space.  \n",
    "That's probably not as weird as it sounds for the clear-cut cases, but the concern of course is whether there is sufficient \"distance\" between passing and non-passing students to allow for proper classification.  If there's significant spatial overlap, it may be difficult for KNeighbors to correctly classify border cases.  This increases the percentage of students who would be susceptible to Type 1 neglect or Type 2 over-worry.  \n",
    "In the alternative, it could be that there is a significant spatial boundary between students who require and students who do not require early intervention, in which case the KNeighbors algorithm should function exceptionally well on this dataset.\n",
    "<br><br>\n",
    "<font color=\"darkred\">_Discarded candidates_</font>:\n",
    "<font color=\"darkred\">**Gaussian Naive Bayes**</font> is an attempt to provide classifications based on the \"naive\" assumption of conditional independence between the input features.  Here, that seems inappropriate, considering that many of the features are likely outgrowths of other, simpler features (e.g. famsize, address, Medu, Fedu, Mjob, Fjob, activities, nursery, internet, goout, health, etc. likely co-vary with parental income).\n",
    "\n",
    "<font color=\"darkred\">**Stochastic Gradient Descent**</font> models attempt to discover the set of parameters which minimize the error of the prediction.  However, they do not provide information on what the error \"surface\" looks like.  That is, (1) there is no guarantee that the minimum which is found is the global, rather than merely a local, minimum, (2) there is no guarantee that the minimum which is found is even \"good\" relative to other regional minimums, and (3) it could be that the error surface is like a rolling sand dune, where no particular minimum is good, or even much better than other possible minimums.  It is possible (and quite effective), through multiple randomized initiations, to merely take thousands of samples of the existing local minimums, and to compare them to each other, in an attempt to locate the global minimum, but this may be computationally very expensive.\n",
    "\n",
    "<font color=\"darkred\">**Support Vector Machines**</font> rely on linear or near-linear separability of the data (as represented in some-dimensional space).  If the data is not linearly separable in the default space, it requires a transformation via a kernel method into a space in which it is linearly separable.  The main problem with that fact is that here I don't know, _a priori_, how the features affect whether a student passes or doesn't.  So, if the default spatial representation of the data results in a poorly-performing SVM, I wouldn't actually know which kernel method to apply to increase performance, even acknowledging that there probably is one which would allow for excellent SVM performance.  Visualizing data in a three-dimensional feature space is hard enough, but here we start with a 30-dimensional feature space, which my tiny human brain struggles to comprehend visually.\n",
    "\n",
    "<font color=\"darkred\">**Logistic Regression**</font> (actually a classification) is a lot like linear regression, except that the continuous variable it outputs is the _likelihood_ of whether a datapoint does or doesn't (in a binary sense) have a label.  Here, that would mean that a properly-implemented Logistic Regression would output the likelihood that a student does or doesn't need early intervention.  The output would be useful, but the underlying assumption is that the variables in the feature space have a consistent linear relation to our output label, which for many of the labels seems like a ludicrous assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "Run the code cell below to initialize three helper functions which you can use for training and testing the three supervised learning models you've chosen above. The functions are as follows:\n",
    "- `train_classifier` - takes as input a classifier and training data and fits the classifier to the data.\n",
    "- `predict_labels` - takes as input a fit classifier, features, and a target labeling and makes predictions using the F<sub>1</sub> score.\n",
    "- `train_predict` - takes as input a classifier, and the training and testing data, and performs `train_clasifier` and `predict_labels`.\n",
    " - This function will report the F<sub>1</sub> score for both the training and testing data separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_classifier(clf, X_train, y_train):\n",
    "    ''' Fits a classifier to the training data. '''\n",
    "    \n",
    "    # Start the clock, train the classifier, then stop the clock\n",
    "    start = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end = time()\n",
    "    \n",
    "    # Print the results\n",
    "    print \"Trained model in {:.4f} seconds\".format(end - start)\n",
    "\n",
    "    \n",
    "def predict_labels(clf, features, target):\n",
    "    ''' Makes predictions using a fit classifier based on F1 score. '''\n",
    "    \n",
    "    # Start the clock, make predictions, then stop the clock\n",
    "    start = time()\n",
    "    y_pred = clf.predict(features)\n",
    "    end = time()\n",
    "    \n",
    "    # Print and return results\n",
    "    print \"Made predictions in {:.4f} seconds.\".format(end - start)\n",
    "    return f1_score(target.values, y_pred, pos_label='yes')\n",
    "\n",
    "\n",
    "def train_predict(clf, X_train, y_train, X_test, y_test):\n",
    "    ''' Train and predict using a classifer based on F1 score. '''\n",
    "    \n",
    "    # Indicate the classifier and the training set size\n",
    "    print \"Training a {} using a training set size of {}. . .\".format(clf.__class__.__name__, len(X_train))\n",
    "    \n",
    "    # Train the classifier\n",
    "    train_classifier(clf, X_train, y_train)\n",
    "    \n",
    "    # Print the results of prediction for both training and testing\n",
    "    print \"F1 score for training set: {:.4f}.\".format(predict_labels(clf, X_train, y_train))\n",
    "    print \"F1 score for test set: {:.4f}.\".format(predict_labels(clf, X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation: Model Performance Metrics\n",
    "With the predefined functions above, you will now import the three supervised learning models of your choice and run the `train_predict` function for each one. Remember that you will need to train and predict on each classifier for three different training set sizes: 100, 200, and 300. Hence, you should expect to have 9 different outputs below â€” 3 for each model using the varying training set sizes. In the following code cell, you will need to implement the following:\n",
    "- Import the three supervised learning models you've discussed in the previous section.\n",
    "- Initialize the three models and store them in `clf_A`, `clf_B`, and `clf_C`.\n",
    " - Use a `random_state` for each model you use, if provided.\n",
    " - **Note:** Use the default settings for each model â€” you will tune one specific model in a later section.\n",
    "- Create the different training set sizes to be used to train each model.\n",
    " - *Do not reshuffle and resplit the data! The new training points should be drawn from `X_train` and `y_train`.*\n",
    "- Fit each model with each training set size and make predictions on the test set (9 in total).  \n",
    "**Note:** Three tables are provided after the following code cell which can be used to store your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a DecisionTreeClassifier using a training set size of 100. . .\n",
      "Trained model in 0.0016 seconds\n",
      "Made predictions in 0.0003 seconds.\n",
      "F1 score for training set: 1.0000.\n",
      "Made predictions in 0.0005 seconds.\n",
      "F1 score for test set: 0.6552.\n",
      "\n",
      "Training a DecisionTreeClassifier using a training set size of 200. . .\n",
      "Trained model in 0.0040 seconds\n",
      "Made predictions in 0.0006 seconds.\n",
      "F1 score for training set: 1.0000.\n",
      "Made predictions in 0.0003 seconds.\n",
      "F1 score for test set: 0.7500.\n",
      "\n",
      "Training a DecisionTreeClassifier using a training set size of 300. . .\n",
      "Trained model in 0.0035 seconds\n",
      "Made predictions in 0.0005 seconds.\n",
      "F1 score for training set: 1.0000.\n",
      "Made predictions in 0.0004 seconds.\n",
      "F1 score for test set: 0.6613.\n",
      "\n",
      "Training a GradientBoostingClassifier using a training set size of 100. . .\n",
      "Trained model in 0.0828 seconds\n",
      "Made predictions in 0.0011 seconds.\n",
      "F1 score for training set: 1.0000.\n",
      "Made predictions in 0.0012 seconds.\n",
      "F1 score for test set: 0.7519.\n",
      "\n",
      "Training a GradientBoostingClassifier using a training set size of 200. . .\n",
      "Trained model in 0.1018 seconds\n",
      "Made predictions in 0.0014 seconds.\n",
      "F1 score for training set: 0.9964.\n",
      "Made predictions in 0.0006 seconds.\n",
      "F1 score for test set: 0.7591.\n",
      "\n",
      "Training a GradientBoostingClassifier using a training set size of 300. . .\n",
      "Trained model in 0.1198 seconds\n",
      "Made predictions in 0.0013 seconds.\n",
      "F1 score for training set: 0.9739.\n",
      "Made predictions in 0.0007 seconds.\n",
      "F1 score for test set: 0.7794.\n",
      "\n",
      "Training a KNeighborsClassifier using a training set size of 100. . .\n",
      "Trained model in 0.0007 seconds\n",
      "Made predictions in 0.0021 seconds.\n",
      "F1 score for training set: 0.8060.\n",
      "Made predictions in 0.0022 seconds.\n",
      "F1 score for test set: 0.7246.\n",
      "\n",
      "Training a KNeighborsClassifier using a training set size of 200. . .\n",
      "Trained model in 0.0010 seconds\n",
      "Made predictions in 0.0052 seconds.\n",
      "F1 score for training set: 0.8800.\n",
      "Made predictions in 0.0029 seconds.\n",
      "F1 score for test set: 0.7692.\n",
      "\n",
      "Training a KNeighborsClassifier using a training set size of 300. . .\n",
      "Trained model in 0.0013 seconds\n",
      "Made predictions in 0.0110 seconds.\n",
      "F1 score for training set: 0.8809.\n",
      "Made predictions in 0.0040 seconds.\n",
      "F1 score for test set: 0.7801.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Import the three supervised learning models from sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# TODO: Initialize the three models\n",
    "clf_A = DecisionTreeClassifier(random_state=42)\n",
    "clf_B = GradientBoostingClassifier(random_state=42)\n",
    "clf_C = KNeighborsClassifier()\n",
    "    # Presumes, without justification, NNeighbors = 5\n",
    "\n",
    "# TODO: Set up the training set sizes\n",
    "X_train_100 = X_train[:100]\n",
    "y_train_100 = y_train[:100]\n",
    "\n",
    "X_train_200 = X_train[:200]\n",
    "y_train_200 = y_train[:200]\n",
    "\n",
    "X_train_300 = X_train\n",
    "y_train_300 = y_train\n",
    "\n",
    "# TODO: Execute the 'train_predict' function for each classifier and each training set size\n",
    "# train_predict(clf, X_train, y_train, X_test, y_test)\n",
    "for clf in [clf_A, clf_B, clf_C]:\n",
    "    for xtrain, ytrain in [(X_train_100,y_train_100),\n",
    "                           (X_train_200,y_train_200),\n",
    "                           (X_train_300,y_train_300)]:\n",
    "        train_predict(clf, xtrain, ytrain, X_test, y_test)\n",
    "        print\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabular Results\n",
    "Edit the cell below to see how a table can be designed in [Markdown](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet#tables). You can record your results from above in the tables provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Classifer 1 - DecisionTreeClassifier**  \n",
    "\n",
    "| Training Set Size | Training Time | Prediction Time (test) | F1 Score (train) | F1 Score (test) |\n",
    "| :---------------: | :---------------------: | :--------------------: | :--------------: | :-------------: |\n",
    "| 100               |        0.0016           |     0.0003             |   1.0000         |    0.6552       |\n",
    "| 200               |        0.0040           |     0.0006             |   1.0000         |    0.7500       |\n",
    "| 300               |        0.0035           |     0.0005             |   1.0000         |    0.6613       |\n",
    "\n",
    "** Classifer 2 - GradientBoostClassifier**  \n",
    "\n",
    "| Training Set Size | Training Time | Prediction Time (test) | F1 Score (train) | F1 Score (test) |\n",
    "| :---------------: | :---------------------: | :--------------------: | :--------------: | :-------------: |\n",
    "| 100               |     0.0828              |    0.0011              |    1.0000        |   0.7519        |\n",
    "| 200               |     0.1018              |    0.0014              |    0.9964        |   0.7591        |\n",
    "| 300               |     0.1198              |    0.0013              |    0.9739        |   0.7794        |\n",
    "\n",
    "** Classifer 3 - KNeighborsClassifier**  \n",
    "\n",
    "| Training Set Size | Training Time | Prediction Time (test) | F1 Score (train) | F1 Score (test) |\n",
    "| :---------------: | :---------------------: | :--------------------: | :--------------: | :-------------: |\n",
    "| 100               |     0.0007              |    0.0021              |   0.8060         |  0.7246         |\n",
    "| 200               |     0.0010              |    0.0052              |   0.8800         |  0.7692         |\n",
    "| 300               |     0.0013              |    0.0110              |   0.8809         |  0.7801         |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the Best Model\n",
    "In this final section, you will choose from the three supervised learning models the *best* model to use on the student data. You will then perform a grid search optimization for the model over the entire training set (`X_train` and `y_train`) by tuning at least one parameter to improve upon the untuned model's F<sub>1</sub> score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 - Choosing the Best Model\n",
    "*Based on the experiments you performed earlier, in one to two paragraphs, explain to the board of supervisors what single model you chose as the best model. Which model is generally the most appropriate based on the available data, limited resources, cost, and performance?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal here is to find the best-performing model.\n",
    "\n",
    "That goal is similar to finding the best-performing student:  We ask the model to complete a test, then we score it on how well it did.  The test is similar to, but not the same as, the study materials we gave to the model ahead of time, because we want the model to be able to generalize its lessons outside of specific problems we assigned it.\n",
    "\n",
    "Score-wise, the \"best\" model is a tie between the **GradientBoostingClassifier** and the **KNeighborsClassifier**, both scoring just around 78% on a preliminary assessment (way better than random guessing!)\n",
    "\n",
    "Other valid considerations are how expensive it is to \"teach\" the model, and how expensive it is for the model to provide us with a meaningful answer.  In this application, this model will likely be \"taught\" once, then asked for meaningful answers numerous times.  So here, we care way more about the expense of the answer than we do about the expense of the learning.\n",
    "\n",
    "In this respect, the **GradientBoostingClassifier** is the clear winner: its prediction time (the expense of the answer) is ~1/10 the prediction time of the KNeighborsClassifier.  The training time (the \"learning\" expense) of the GradientBoostingClassifier is ~100x that of the KNeighborsClassifier, but that's a one-time occurrence and thus is far less of a concern than the expense of an answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 - Model in Layman's Terms\n",
    "*In one to two paragraphs, explain to the board of directors in layman's terms how the final model chosen is supposed to work. Be sure that you are describing the major qualities of the model, such as how the model is trained and how the model makes a prediction. Avoid using advanced mathematical or technical jargon, such as describing equations or discussing the algorithm implementation.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want all of the students to graduate.  A major obstacle to graduation is not receiving timely academic intervention.  Knowing earlier which students need intervention to pass is thus a top priority.\n",
    "\n",
    "By applying machine learning techniques, we are able to accurately predict which students require academic intervention in order to pass, solely by examining information from the student's academic record supplemented with information provided by a student survey.\n",
    "\n",
    "This prediction is possible because whether or not a student needs intervention is not random, but rather depends quantifiably on a variety of other factors such as parental involvement and education, extracurricular activities, school absences, etc.  By examining which students have failed in the past, this model is able to understand which students are currently at risk of failing.\n",
    "\n",
    "At regular intervals, this model will examine each enrolled student for signs that the student may need intervention in order to pass.  The model will flag students that are at risk of failing without intervention, so that the administration can intervene before it is too late to help the student."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation: Model Tuning\n",
    "Fine tune the chosen model. Use grid search (`GridSearchCV`) with at least one important parameter tuned with at least 3 different values. You will need to use the entire training set for this. In the code cell below, you will need to implement the following:\n",
    "- Import [`sklearn.grid_search.gridSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html) and [`sklearn.metrics.make_scorer`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html).\n",
    "- Create a dictionary of parameters you wish to tune for the chosen model.\n",
    " - Example: `parameters = {'parameter' : [list of values]}`.\n",
    "- Initialize the classifier you've chosen and store it in `clf`.\n",
    "- Create the F<sub>1</sub> scoring function using `make_scorer` and store it in `f1_scorer`.\n",
    " - Set the `pos_label` parameter to the correct value!\n",
    "- Perform grid search on the classifier `clf` using `f1_scorer` as the scoring method, and store it in `grid_obj`.\n",
    "- Fit the grid search object to the training data (`X_train`, `y_train`), and store it in `grid_obj`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 243 candidates, totalling 2430 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done 256 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=2)]: Done 1156 tasks      | elapsed:   30.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made predictions in 0.0011 seconds.\n",
      "Tuned model has a training F1 score of 0.8419.\n",
      "Made predictions in 0.0014 seconds.\n",
      "Tuned model has a testing F1 score of 0.7733.\n",
      "Tuned model has the following parameters: {'max_features': 0.042, 'n_estimators': 69, 'learning_rate': 0.037, 'max_depth': 3, 'min_samples_leaf': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done 2430 out of 2430 | elapsed:  1.1min finished\n"
     ]
    }
   ],
   "source": [
    "# TODO: Import 'GridSearchCV' and 'make_scorer'\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# TODO: Create the parameters list you wish to tune\n",
    "parameters = {\n",
    "    \"learning_rate\":[0.036, 0.037, 0.038]\n",
    "    ,\"n_estimators\":[68, 69, 70]\n",
    "    ,\"max_depth\":[2,3,4]\n",
    "    ,\"min_samples_leaf\":[2,3,4]\n",
    "    ,\"max_features\":[0.041, 0.042, 0.043]\n",
    "    #,\"min_samples_split\":[2]\n",
    "    #,\"min_weight_fraction_leaf\":[0.]     # These 4 parameters tended never to increase score, \n",
    "    #,\"subsample\":[1.]                    # hence are left at their defaults.\n",
    "    #,\"max_leaf_nodes\":[None]\n",
    "}\n",
    "\n",
    "# TODO: Initialize the classifier\n",
    "clf = GradientBoostingClassifier(random_state = 42)\n",
    "\n",
    "# TODO: Make an f1 scoring function using 'make_scorer' \n",
    "f1_scorer = make_scorer(f1_score, pos_label=\"yes\")\n",
    "\n",
    "# TODO: Perform grid search on the classifier using the f1_scorer as the scoring method\n",
    "grid_obj = GridSearchCV(clf, parameters, scoring=f1_scorer, verbose=1, n_jobs=1, cv=10)\n",
    "\n",
    "# TODO: Fit the grid search object to the training data and find the optimal parameters\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Get the estimator\n",
    "clf = grid_obj.best_estimator_\n",
    "\n",
    "# Report the final F1 score for training and testing after parameter tuning\n",
    "print \"Tuned model has a training F1 score of {:.4f}.\".format(predict_labels(clf, X_train, y_train))\n",
    "print \"Tuned model has a testing F1 score of {:.4f}.\".format(predict_labels(clf, X_test, y_test))\n",
    "\n",
    "# Report the parameters of the best estimator\n",
    "print \"Tuned model has the following parameters: \" + str(grid_obj.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83303022470972765"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_obj.best_score_\n",
    "# For my own satisfaction, print the f1_score from the \"best\" classifier in the 10-fold cross-validation grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5 - Final F<sub>1</sub> Score\n",
    "*What is the final model's F<sub>1</sub> score for training and testing? How does that score compare to the untuned model?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer: **\n",
    "\n",
    "The final model's F<sub>1</sub> score for training was 0.8419.  \n",
    "The final model's F<sub>1</sub> score for testing was 0.7733.\n",
    "\n",
    "Actually, this is <font color=\"darkred\">slightly worse</font> than the testing score for the untuned model (whose F<sub>1,testing</sub> score was 0.7794\n",
    "\n",
    "It's unclear to me, however, whether that difference is significant, in a statistical sense.\n",
    "\n",
    "Further, it's unclear to me whether the final model is stuck in a local minimum of performance, in which case there are other pockets of the parameter-space that would lead to better performance on the testing set.\n",
    "\n",
    "Further, it's unclear to me whether the preliminary model was simply \"lucky\" on the testing set, especially since the previously-defined <font color=\"burntorange\">train_classifier</font> and `train_predict` methods do not involve cross-validation methods.\n",
    "\n",
    "Further, it's unclear to me how I should interpret the F<sub>1</sub> scores for testing relative to the `grid_obj.best_score_` attribute, which takes into account cross-validation across k-folds (here 10)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: Once you have completed all of the code implementations and successfully answered each question above, you may finalize your work by exporting the iPython Notebook as an HTML document. You can do this by using the menu above and navigating to  \n",
    "**File -> Download as -> HTML (.html)**. Include the finished document along with this notebook as your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python2",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
